> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [blog.csdn.net](https://blog.csdn.net/weixin_43334693/article/details/129087464?spm=1001.2014.3001.5501)

![](https://img-blog.csdnimg.cn/0d7832425c364059b651fa45d8b29c72.gif)
---------------------------------------------------------------------

前言
--

时隔一年，[YOLOv2](https://so.csdn.net/so/search?q=YOLOv2&spm=1001.2101.3001.7020) 隆重登场，新的 YOLO 版本论文叫《YOLO9000: Better, Faster, Stronger》，作者 Joseph Redmon 和 Ali Farhadi 在 YOLOv1 的基础上，进行了大量改进，提出了 YOLOv2 和 YOLO9000，重点解决 YOLOv1 召回率和定位精度方面的不足。

论文原文：[[1612.08242] YOLO9000: Better, Faster, Stronger (arxiv.org)](https://arxiv.org/abs/1612.08242 "[1612.08242] YOLO9000: Better, Faster, Stronger (arxiv.org)")

项目主页：[YOLO: Real-Time Object Detection (pjreddie.com)](https://pjreddie.com/darknet/yolo/ "YOLO: Real-Time Object Detection (pjreddie.com)") 

前情回顾：[【YOLO 系列】YOLOv1 论文超详细解读（翻译 ＋学习笔记）](https://blog.csdn.net/weixin_43334693/article/details/129011644?spm=1001.2014.3001.5501 "【YOLO系列】YOLOv1论文超详细解读（翻译 ＋学习笔记）")

**目录**
------

[前言](#%E5%89%8D%E8%A8%80) 

[Abstract—摘要](#Abstract%E2%80%94%E6%91%98%E8%A6%81) 

[一、 Introduction—引言](#%C2%A0%E4%B8%80%E3%80%81%20Introduction%E2%80%94%E5%BC%95%E8%A8%80)

[二、 Better—更好](#%E4%BA%8C%E3%80%81%20Better%E2%80%94%E6%9B%B4%E5%A5%BD%C2%A0)  

[2.1 Batch Normalization—批量归一化](#t2)

[2.2 High Resolution Classifier—高分辨率分类器](#t3)

[2.3 Convolutional With Anchor Boxes—带有 Anchor Boxes 的卷积](#t4)

[2.4 Dimension Clusters—维度聚类（K-means 聚类确定 Anchor 初始值）](#t5)

[2.5 Direct location prediction—直接的位置预测](#t6)

[2.6 Fine-Grained Features—细粒度的特征](#t7)

[2.7 Multi-Scale Training—多尺度的训练](#t8)

[2.8 Further Experiments—进一步的实验](#t9)

 [三、Faster—更快](#%E4%B8%89%E3%80%81Faster%E2%80%94%E6%9B%B4%E5%BF%AB)

[3.1 Darknet-19](#t10)

[3.2 Training for classification—分类的训练](#t11)

[3.3 Training for detection—检测的训练](#t12)

[四、Stronger—更强（YOLO9000 部分）](#%E5%9B%9B%E3%80%81Stronger%E2%80%94%E6%9B%B4%E5%BC%BA%EF%BC%88YOLO9000%E9%83%A8%E5%88%86%EF%BC%89) 

[4.1 Hierarchical classification—分层分类](#t13)

[4.2 Dataset combination with WordTree—用 WordTree 组合数据集](#t14)

[4.3 Joint classification and detection—联合分类和检测](#t15)

[五、Conclusion—结论](#%E4%BA%94%E3%80%81Conclusion%E2%80%94%E7%BB%93%E8%AE%BA)

Abstract—摘要
-----------

### 翻译

我们介绍了 YOLO9000，一个最先进的实时目标检测系统，可以检测超过 9000 个目标类别。首先，我们提出了对 [YOLO](https://so.csdn.net/so/search?q=YOLO&spm=1001.2101.3001.7020) 检测方法的各种改进，这些改进既是新的，也是来自先前的工作。改进后的模型 YOLOv2 在标准检测任务上是最先进的，如 PASCAL VOC 和 COCO。使用一种新的、多尺度的训练方法，同一个 YOLOv2 模型可以在不同的规模下运行，在速度和准确性之间提供了一个简单的权衡。在 67FPS 时，YOLOv2 在 VOC 2007 上得到 76.8mAP。在 40 FPS 时，YOLOv2 得到 78.6 mAP，超过了最先进的方法，如带有 ResNet 和 SSD 的 Faster R-CNN，同时运行速度仍然很高。最后，我们提出了一种联合训练目标检测和分类的方法。使用这种方法，我们在 COCO 检测数据集和 ImageNet 分类数据集上同时训练 YOLO9000。我们的联合训练使 YOLO9000 能够预测没有标记检测数据的目标类别的检测情况。我们在 ImageNet 检测任务上验证了我们的方法。尽管只有 200 个类中的 44 个有检测数据，YOLO9000 在 ImageNet 检测验证集上得到了 19.7 的 mAP。在 COCO 上没有的的 156 个类中，YOLO9000 得到了 16.0 mAP。但 YOLO 能检测的不仅仅是 200 个类；它能预测 9000 多个不同目标类别的检测。而且它仍然是实时运行的。

### 精读 

#### YOLOv1 的不足

（1）定位不准确

（2）和基于 region proposal 的方法相比召回率较低。

#### 本文的改进

*   **YOLO9000：** 先进，实时的目标检测方法，可检测 9000 多类物体
*   **多尺度训练方法（ multi-scale training）：** 相同的 YOLOv2 模型可以在不同的大小下运行，在速度和精度之间提供了一个简单的折中
*   **mAP 表现更好：** 67FPS，在 VOC 2007 上 76.8 mAP，在 40 FPS，78.6mAP；而且速度更快。
*   **提出了一种联合训练目标检测和分类的方法：** 使用该方法在 COCO 目标检测数据集和 Imagenet 图像分类数据集上，训练出了 YOLO9000
*   **可以检测出更多的类别：** 即使这些类别没有在目标检测的数据集中出现

 **一、 Introduction—引言**
-----------------------

### **翻译**

通用的目标检测应该是快速、准确的，并且能够识别各种各样的目标。自从引入神经网络以来，检测框架已经变得越来越快和准确。然而，大多数检测方法仍然被限制在一小部分目标上。

与分类和标记等其他任务的数据集相比，当前的目标检测数据集是有限的。最常见的检测数据集包含几千到几十万张图像，有几十到几百个标签 [3] [10] [2]。分类数据集有数以百万计的图像，有数万或数十万个类别 [20] [2]。

我们希望检测能够达到目标分类的水平。然而，为检测而给图像贴标签比为分类或标记贴标签要昂贵得多（标签通常是给用户免费提供的）。因此，我们不太可能在不久的将来看到与分类数据集相同规模的检测数据集。

我们提出了一种新的方法来利用我们已经拥有的大量分类数据，并利用它来扩大当前检测系统的范围。我们的方法使用目标分类的分层观点，使我们能够将不同的数据集结合在一起。

我们还提出了一种联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位目标，同时使用分类图像来增加其词汇量和鲁棒性。

使用这种方法，我们训练了 YOLO9000，一个实时的目标检测器，可以检测超过 9000 个不同的物体类别。首先，我们在基础 YOLO 检测系统的基础上进行改进，以产生 YOLOv2，一个最先进的实时检测器。然后，我们使用我们的数据集组合方法和联合训练算法，在 ImageNet 的 9000 多个类别以及 COCO 的检测数据上训练一个模型。

我们所有的代码和预训练的模型都可以在线获得：http://pjreddie.com/yolo9000/。

### 精读 

#### 目标检测现状的不足

*   当前的目标检测数据集是有限的
*   目标检测能检测的对象种类非常有限，可检测的物体少

#### 本文工作

（1）**使用联合数据集：** 利用已有的分类数据集，来拓展目标检测的范围。利用对象分类的分层视图，使得可以将不同数据集组合到一起

（2）**提出联合训练算法：** 可以在检测数据集和分类数据集上训练目标分类器，用标记的目标检测数据集优化定位精度，利用分类图像来增加其词汇量鲁棒性

（3）**改进 YOLOv1 提出 YOLOv2：** 一种最先进的实时检测器

（4）**提出 YOLO9000：** 先将 YOLO 优化成 YOLOv2，再用联合方法训练出 YOLO9000

### **二、 Better—更好** 

### 2.1 Batch Normalization—批量归一化

### 翻译

相比于最先进的检测系统，YOLO 存在着各种缺陷。与 [Faster](https://so.csdn.net/so/search?q=Faster&spm=1001.2101.3001.7020) R-CNN 相比，对 YOLO 的错误分析表明，YOLO 出现了大量的定位错误。此外，与基于区域建议的方法相比，YOLO 的召回率相对较低。因此，我们主要关注的是在保持分类精度的同时，提高召回率和定位准确度。

计算机视觉通常趋向于更大、更深的网络 [6] [18] [17]。更好的性能往往取决于训练更大的网络或将多个模型集合在一起。然而，在 YOLOv2 中，我们希望有一个更准确的检测器，但仍然是快速的。我们没有扩大我们的网络，而是简化了网络，然后让表征更容易学习。我们将过去工作中的各种想法与我们自己的新概念结合起来，以提高 YOLO 的性能。在表 2 中可以看到结果的总结。

**批量归一化** 批量归一化导致收敛性的显著改善，同时消除了对其他形式的规范化的需求 [7]。通过在 YOLO 的所有卷积层上添加批量归一化，我们在 mAP 上得到了超过 2% 的改善。批量规范化也有助于规范化模型。有了批归一化，我们可以在不过拟合的情况下去除模型中的 dropout。

### 精读

#### 目的

CNN 在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，对网络的每一层的输入 (每个卷积层后) 都做了归一化，**这样网络就不需要每层都去学数据的分布，收敛会更快**。

#### 方法

在 YOLO 模型的所有卷积层上添加 **Batch Normalization**。

![](https://img-blog.csdnimg.cn/ef0054c083c14c3c8a6fec09bfa7a13e.png)

 （图片来源：同济子豪兄）

#### 效果

mAP 获得了 2% 的提升。Batch Normalization 也有助于规范化模型，可以在舍弃 dropout 优化后依然不会过拟合。

### 2.2 High Resolution Classifier—高分辨率分类器

### 翻译

**高分辨率分类器** 所有最先进的检测方法都使用在 ImageNet 上预训练的分类器 [16]。从 AlexNet 开始，大多数分类器在小于 256×256 的输入图像上运行 [8]。最初的 YOLO 在 224×224 的情况下训练分类器网络，并将分辨率提高到 448 以进行检测训练。这意味着网络在切换到检测学习时还必须调整到新的输入分辨率。

对于 YOLOv2，我们首先在 ImageNet 上以 448×448 的完整分辨率对分类网络进行微调，并进行 10 个 epoch。这让网络有时间调整其滤波器，以便在更高的分辨率输入下更好地工作。然后，我们再对检测网络的结果微调。这个高分辨率的分类网络使我们的 mAP 增加了近 4%。

### 精读

#### 分类器介绍

检测方法都使用在 ImageNet 上预先训练的分类器作为预训练模型。从 AlexNet 开始，大多数分类器的输入都小于 256×256。

#### v1 中的使用

v1 中预训练使用的是分类数据集，大小是 224×224 ，然后迁移学习，微调时使用 YOLO 模型做目标检测的时候才将输入变成 448 × 448。这样改变尺寸，网络就要多重新学习一部分，会带来性能损失。

#### v2 中的改进

v2 直接在预训练中输入的就是 **448×448** 的尺寸，微调的时候也是 **448 × 448**。

#### 效果

使 mAP 增加了近 4%

###  2.3 Convolutional With Anchor Boxes—带有 Anchor Boxes 的卷积

### 翻译

**带有锚框的卷积** YOLO 直接使用卷积特征提取器顶部的全连接层来预测边界框的坐标。Faster R-CNN 不直接预测坐标，而是使用手工挑选的先验因素来预测边界框 [15]。Faster R-CNN 中的区域生成网络（RPN）只使用卷积层来预测锚框的偏移量和置信度。由于预测层是卷积，RPN 预测了特征图中每个位置的偏移量。预测偏移量而不是坐标可以简化问题，使网络更容易学习。

我们从 YOLO 中移除全连接层，并使用锚框来预测边界框。首先，我们消除了一个池化层，使网络卷积层的输出具有更高的分辨率。我们还缩小了网络，使其在分辨率为 416×416 的输入图像上运行，而不是 448×448。我们这样做是因为我们希望在我们的特征图中有奇数个位置，以便只有一个中心单元。目标，尤其是大型目标，往往会占据图像的中心位置，所以在中心位置有一个单一的位置来预测这些目标是很好的，而不是在中心附近的四个位置。YOLO 的卷积层对图像进行了 32 倍的降样，所以通过使用 416 的输入图像，我们得到了一个 13×13 的输出特征图。

引入锚框后，我们将类别预测机制与空间位置分开处理，单独预测每个锚框的类和目标。和原来的 YOLO 一样，目标预测仍然预测先验框和真实框的 IOU，而类别预测则预测在有目标存在下，该类别的条件概率。

使用锚框，我们得到的准确率会有小幅下降。YOLO 每张图片只预测了 98 个框，但使用锚框后，我们的模型预测了超过一千个框。在没有锚框的情况下，我们的中间模型 mAP 为 69.5，召回率为 81%。有了锚框，我们的模型 mAP 为 69.2，召回率为 88%。即使 mAP 下降了，平均召回率的增加意味着我们的模型有更大的改进空间。

### 精读

#### 什么是 Anchor？

**定义：** Anchor（先验框） 就是一组预设的边框，在训练时，以真实的边框位置相对于预设边框的偏移来构建训练样本。 这就相当于，预设边框先大致在可能的位置 “框” 出来目标，然后再在这些预设边框的基础上进行调整。简言之就是在图像上预设好的不同大小，不同长宽比的参照框。

**Anchor Box：** 一个 Anchor Box 可以由边框的纵横比和边框的面积（尺度) 来定义，相当于一系列预设边框的生成规则，根据 Anchor Box，可以在图像的任意位置，生成一系列的边框。由于 Anchor Box 通常是以 CNN 提取到的 Feature Map 的点为中心位置，生成边框，所以一个 Anchor Box 不需要指定中心位置。

#### **Anchor Box 的构成**

*   使用 CNN 提取的 Feature Map 的点，来定位目标的位置。
*   使用 Anchor Box 的 Scale 来表示目标的大小。
*   使用 Anchor Box 的 Aspect Ratio 来表示目标的形状。

#### 之前研究

**YOLOv1:** 使用全连接层来直接预测边界框（x,y,w,h,c）其中边界框的坐标是相对于 cell 的，宽与高是相对于整张图片。由于各个图片中存在不同尺度和长宽比的物体，YOLOv1 在训练过程中学习适应不同物体的形状是比较困难的，这也导致 YOLOv1 在精确定位方面表现较差。

**Faster R-CNN：** 不是直接预测目标边界框，而是使用手工挑选的先验 Anchor Boxes。利用 RPN 预测的边界框是相对于 Anchor Boxes 的坐标和高宽的偏移 offset。RPN 在特征图的每个位置预测 Anchor Box 偏移量而不是坐标，简化了问题，使网络更容易学习。

#### YOLOv2 的改进

（1）删掉全连接层和最后一个 pooling 层，使得最后的卷积层可以有更高分辨率的特征

（2）缩小网络操作的输入图像为 416×416

> Q：为什么是 416×416，而不是 448×448？
> 
> YOLOv2 模型下采样的总步长为 32, 对于 416×416 大小的图片，最终得到的特征图大小为 13×13（416/32=13），特征图中有奇数个位置，所以只有一个中心单元格。物体往往占据图像的中心，所以最好在中心有一个单独的位置来预测这些物体，而不是在附近的四个位置。
> 
> ![](https://img-blog.csdnimg.cn/ee0ac17938bc4c44862912152c124697.png)

（3）使用 Anchor Boxes

#### 效果

使用 Anchor，模型的 mAP 值从 69.5 降到了 69.2，下降了一丢丢，而召回率却从 81% 提高到了 88%。

> Q：精度（precision）和召回率（recall）：
> 
> **precision：** 预测框中包含目标的比例。
> 
> **recall：** 真正目标被检测出来的比例。
> 
> 简言之，recall 表示得找全，precision 表示得找准。

#### v2 和 v1 对比

<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td></td><td><strong>YOLOv1</strong></td><td><strong>YOLOv2</strong></td></tr><tr><td><strong>初始设置</strong></td><td>初始生成两个 boxes，加大了学习复杂度。</td><td>Anchor 初始是固定的，但在训练过程中会进行微调。使用 Anchor boxes 之后，每个位置的各个 Anchor box 都单独预测一组分类概率值。</td></tr><tr><td><strong>输出公式</strong></td><td>(框数 * 信息数)+ 分类数</td><td>框数 *(信息数 + 分类数)</td></tr><tr><td><strong>公式含义</strong></td><td>在 YOLOv1 中，类别概率是由 grid cell 来预测的，每个 cell 都预测 2 个 boxes，每个 boxes 包含 5 个值，每个 grid cell 携带的是 30 个信息。但是每个 cell 只预测一组分类概率值，供 2 个 boxes 共享。</td><td>在 YOLOv2 中，类别概率是属于 box 的，每个 box 对应一个类别概率，而不是由 cell 决定，因此这边每个 box 对应 25 个预测值。每个 grid cell 携带的是 25 × 5 =125 个信息，25 是 xywh + 置信度 + 分类数，5 就是 5 个 Anchor。</td></tr><tr><td><strong>输出框</strong></td><td>7 × 7 × 2 = 98 个框</td><td>13 × 13 × 5 = 845 个框</td></tr><tr><td><strong>输出值</strong></td><td>7 ×7 × 30</td><td>13 × 13 × 5 × 25</td></tr></tbody></table>

![](https://img-blog.csdnimg.cn/0d6b352367b54c02bf856ad3a58d0dda.png)

### 2.4 Dimension Clusters—维度聚类（K-means 聚类确定 Anchor 初始值）

### 翻译

**维度集群** 在与 YOLO 一起使用锚框时，我们遇到了两个问题。第一个问题是，框的尺寸是手工挑选的。网络可以学习适当地调整框，但是如果我们为网络挑选更好的先验锚框来开始，我们可以使网络更容易学习预测好的检测结果。

我们在训练集的边界框上运行 k-means 聚类，以自动找到好的先验参数，而不是手工选择先验参数。如果我们使用标准的 k-means 和欧氏距离，大的框比小的框产生更多的误差。然而，我们真正想要的是能获得好的 IOU 分数的先验锚框，这与框的大小无关。因此，对于距离度量，我们使用：d(box , centroid)=1−IOU( box , centroid )​。

我们对不同的 k 值运行 k-means，并绘制出最接近中心点的平均 IOU，见图 2。我们选择 k = 5 作为模型复杂性和高召回率之间的良好权衡。聚类中心点与手工挑选的锚框有明显不同。短而宽的框较少，高而薄的框较多。

我们在表 1 中比较了我们的聚类策略和手工挑选的锚框的平均 IOU 与最接近的先验。在只有 5 个先验的情况下，中心点的表现与 9 个锚框相似，平均 IOU 分别为 61.0，60.9。如果我们使用 9 个中心点，我们会看到一个高得多的平均 IOU。这表明，使用 k-means 来生成我们的边界框，使模型开始有一个更好的表示，并使任务更容易学习。

### 精读

#### 使用 Anchor 的问题一

Anchor Boxes 的尺寸是手工指定了长宽比和尺寸，相当于一个超参数，这违背了 YOLO 对于目标检测模型的初衷，**因为如果指定了 Anchor 的大小就没办法适应各种各样的物体了**。

#### 解决方法

在训练集的边界框上运行 K-means 聚类训练 bounding boxes，可以自动找到更好的 boxes 宽高维度。由上面分析已知，设置先验 Anchor Boxes 的主要目的是为了使得预测框与真值的 IOU 更好，所以聚类分析时选用 box 与聚类中心 box 之间的 IOU 值作为距离指标

> K-means 算法步骤：
> 
> 1. 选择初始化的 K 个样本作为初始聚类中心
> 
> 2. 针对数据集中每个样本，计算它到 K 个聚类中心的距离，并将其分到距离最小的聚类中心所对应的类中
> 
> 3. 针对每个类别，重新计算它的聚类中心
> 
> 4. 重复上面的步骤 2、3，直到达到某个终止条件（迭代次数、最小误差变化）
> 
> （具体可参见[机器学习算法 ---- 聚类 (K-Means、LVQ、GMM、DBSCAN、AGNES) (学习笔记)](https://blog.csdn.net/qq_38737428/article/details/123967093?spm=1001.2014.3001.5501 "机器学习算法----聚类 (K-Means、LVQ、GMM、DBSCAN、AGNES) (学习笔记)")

**公式：** d(box, centroid) = 1 − IOU(box, centroid) （box: 其他框， centroid：聚类中心框）

如下图，选取不同的 k 值（聚类的个数）运行 K-means 算法，并画出平均 IOU 和 K 值的曲线图。当 k = 5 时，可以很好的权衡模型复杂性和高召回率。与手工挑选的相比，K-means 算法挑选的检测框形状多为瘦高型。

![](https://img-blog.csdnimg.cn/910a00739960473fb4d03c4e8b21b9d3.png)

> Q：为什么不尽量选择大的 k 值？
> 
> 因为 K 越大就生成越多的 Anchor，越多的框自然准确率就能上去了，但同时也成倍的增加了模型的复杂度。R-CNN 就是因为提取 2K 个候选框拉跨的。

### 2.5 Direct location prediction—直接的位置预测

### 翻译

**直接的位置预测。** 当 YOLO 使用锚框时，我们遇到了第二个问题：模型的不稳定性，特别是在早期迭代中。大部分的不稳定性来自于对框的（x，y）位置的预测。在区域生成网络中，网络预测值 tx 和 ty，（x，y）中心坐标的计算方法是：![](https://latex.csdn.net/eq?x%3D%5Cleft%20%28%20t_%7Bx%7D*w_%7Ba%7D%20%5Cright%20%29-x_%7Ba%7D)，![](https://latex.csdn.net/eq?y%3D%5Cleft%20%28%20t_%7By%7D*h_%7Ba%7D%20%5Cright%20%29-y_%7Ba%7D)

例如，tx=1 的预测会将框向右移动，移动的宽度为锚框的宽度，tx=-1 的预测会将框向左移动相同的长度。

这个公式是不受限制的，所以任何锚框都可以在图像中的任何一点结束，而不管这个框是在哪个位置预测的。在随机初始化的情况下，模型需要很长时间才能稳定地预测出合理的偏移量。

我们不预测偏移量，而是遵循 YOLO 的方法，预测相对于网格单元位置的坐标。这使得真实值的界限在 0 到 1 之间。我们使用逻辑激活来约束网络的预测，使其落在 0~1 这个范围内。

网络在输出特征图中的每个单元预测了 5 个边界框。该网络为每个边界框预测了 5 个坐标，即 tx、ty、tw、th 和 to。如果单元格与图像左上角的偏移量为（cx，cy），且先验框的宽度和高度为 pw，ph，则预测值对应于：

![](https://latex.csdn.net/eq?b_%7Bx%7D%3D%5Csigma%20%28t_%7Bx%7D%29&plus;c_%7Bx%7D)

![](https://latex.csdn.net/eq?b_%7By%7D%3D%5Csigma%20%28t_%7By%7D%29&plus;c_%7By%7D)

![](https://latex.csdn.net/eq?b_%7Bw%7D%3Dp_%7Bw%7De%5E%7Bt_%7Bw%7D%7D)

![](https://latex.csdn.net/eq?b_%7Bh%7D%3Dp_%7Bh%7De%5E%7Bt_%7Bh%7D%7D)

![](https://latex.csdn.net/eq?Pr%28object%29*IOU%28b%2Cobject%29%3D%5Csigma%20%28t_%7B0%7D%29)

由于我们限制了位置预测，参数化更容易学习，使网络更稳定。使用维度聚类以及直接预测边界框中心位置，比起使用锚框的版本，YOLO 提高了近 5%。 

### 精读

#### 使用 Anchor 的问题二

模型不稳定，特别是在早期迭代期间。大多数不稳定性来自于对边框 (x, y) 位置的预测。

#### RPN 网络的位置预测

**方法：** 预测相对于 Anchor Box 的坐标的偏移，和相对于 Anchor Box 高宽的偏移。

**计算公式：** 预测框中心坐标 = 输出的偏移量 ×Anchor 宽高 + Anchor 中心坐标

![](https://img-blog.csdnimg.cn/e707a4e0407b436897f922b0a9905118.png)

**不足：** 这个公式是不受约束的，因此任何锚框可以出现在图像中的任何位置。在随机初始化的情况下，模型需要很长时间才能稳定到预测合理的偏移量。

#### **YOLOv2 的改进**

**方法：** 预测边界框中心点相对于对应 cell 左上角位置的相对偏移值。将网格归一化为 1×1，坐标控制在每个网格内，同时配合 sigmod 函数将预测值转换到 0~1 之间的办法，做到每一个 Anchor 只负责检测周围正负一个单位以内的目标 box。

**计算公式：** 一个网格相对于图片左上角的偏移量是 cx，cy。先验框的宽度和高度分别是 pw 和 ph，则预测的边界框相对于特征图的中心坐标 (bx，by) 和宽高 bw、bh

![](https://img-blog.csdnimg.cn/2447f29e18a64c1ca1bd9bcbef138fe9.png)

#### 效果

使模型更容易稳定训练，mAP 值提升了约 5%。

### 2.6 Fine-Grained Features—细粒度的特征

### 翻译

**细粒度的特征**这个修改后的 YOLO 在 13×13 的特征图上预测探测结果。虽然这对大型物体来说是足够的，但它可能会受益于更细粒度的特征来定位较小的物体。Faster R-CNN 和 SSD 都在网络中的各种特征图上运行他们的网络，以获得多个分辨率。我们采取了一种不同的方法，只需要增加一个直通层，从早期的层中提取 26×26 分辨率的特征。

直通层通过将相邻的特征堆叠到不同的通道而不是空间位置上，将高分辨率的特征与低分辨率的特征串联起来，类似于 ResNet 中的恒等映射。这种细粒度的特征。这就把 26×26×512 的特征图变成了 13×13×2048 的特征图，它可以与原始特征连接起来。我们的检测器在这个扩展的特征图之上运行，这样它就可以访问细粒度的特征。这使性能有了 1% 的适度提高。

### 精读

#### 为什么使用细粒特征？

这个修改后的 YOLO 在 13 × 13 特征图上进行检测。虽然这对于大型对象来说已经足够了，但是对于较小的对象来说，更细粒度的特性可能会使得检测效果更好。

#### 使用细粒度特征

**Faster R-CNN 和 SSD：** 使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。

**YOLOv2：** 不同的方法，为网络简单地添加一个直通层（ passthrough layer），获取前层 26×26 分辨率特征。

#### 直通层（ passthrough layer）

*   将相邻的特征叠加到不同的通道来，将高分辨率的特征与低分辨率的特征连接起来
*   将前层 26×26×512 的特征图转换为 13×13×2048 的特征图，并与原最后层特征图进行拼接。

**具体计算过程：** YOLO v2 提取 Darknet-19 最后一个 maxpooling 层的输入，得到 26×26×512 的特征图。经过 1×1×64 的卷积以降低特征图的维度，得到 26×26×64 的特征图，然后经过 pass through 层的处理变成 13x13x256 的特征图（抽取原特征图每个 2x2 的局部区域组成新的 channel，即原特征图大小降低 4 倍，channel 增加 4 倍），再与 13×13×1024 大小的特征图连接，变成 13×13×1280 的特征图，最后在这些特征图上做预测。

**具体操作：**

![](https://img-blog.csdnimg.cn/c4dfa0e97bd0458290668a96f16233fe.png)

一个 feature map，也就是在最后的池化之前，分成两路：一路是做拆分，分成四块，四块拼成一个长条，另一个是做正常的池化卷积操作，最后两个长条叠加输出。

> Q：如何拆分成四块的？
> 
> 并不是简单的 “两刀切 4 块”，而是在每个 2×2 的小区域上都选择左上角块，具体看下图。
> 
> ![](https://img-blog.csdnimg.cn/02886eefe66742afa30c601e778a1c64.png)

**注意：** 这里的叠加不是 ResNet 里的 add，而是拼接，是 DenseNet 里的 concat。

#### 效果

提升了 1% 的 mAP

### 2.7 Multi-Scale Training—多尺度的训练

### 翻译

**多尺度的训练**原始的 YOLO 使用 448×448 的输入分辨率。通过添加锚框，我们将分辨率改为 416×416。然而，由于我们的模型只使用卷积层和池化层，因此可以实时调整大小。我们希望 YOLOv2 能够鲁棒地运行在不同尺寸的图像上，所以我们将多尺度训练应用到模型中。

我们不需要修改输入图像的大小，而是每隔几个迭代就改变网络。每 10 个批次，我们的网络就会随机选择一个新的图像尺寸。由于我们的模型缩减了 32 倍，我们从以下 32 的倍数中抽取：{320, 352, …, 608}。因此，最小的选项是 320 × 320，最大的是 608 × 608。我们将调整网络的尺寸，然后继续训练。

这种制度迫使网络学会在各种输入维度上进行良好的预测。这意味着同一个网络可以预测不同分辨率下的检测结果。网络在较小的尺寸下运行得更快，因此 YOLOv2 在速度和准确性之间提供了一个简单的权衡。

在低分辨率下，YOLOv2 作为一个廉价、相当准确的检测器运行。在 288×288 时，它以超过 90 FPS 的速度运行，其 mAP 几乎与 Faster R-CNN 一样好。这使它成为较小的 GPU、高帧率视频或多个视频流的理想选择。

在高分辨率下，YOLOv2 是一个最先进的检测器，在 VOC 2007 上的 mAP 为 78.6，而运行速度仍高于实时速度。

### 精读

#### YOLOv1

**方法：** 使用 448×448 的固定分辨率输入。

#### YOLOv2 的改进

**原理：** YOLOv2 模型只使用了卷积和池化层，所以可以动态调整输入大小。每隔几次迭代就改变网络，而不是固定输入图像的大小。

**做法：** 网络每 10 批训练后随机选择一个新的图像尺寸大小。由于模型下采样了 32 倍，从以下 32 的倍数 {320,352，…，608} 作为图像维度的选择。将网络输入调整到那个维度，并继续训练。

**作用：** 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在输入 size 较大时，训练速度较慢，在输入 size 较小时，训练速度较快，而 multi-scale training 又可以提高准确率，因此算是准确率和速度都取得一个不错的平衡。

#### YOLOv2 和其他网络成绩对比：

在小尺寸图片检测中，YOLOv2 成绩很好，输入为 228 × 228 的时候，帧率达到 90FPS，mAP 几乎和 Faster R-CNN 的水准相同。使得其在低性能 GPU、高帧率视频、多路视频场景中更加适用。在大尺寸图片检测中，YOLOv2 达到了先进水平，VOC2007 上 mAP 为 78.6%，仍然高于平均水准。

![](https://img-blog.csdnimg.cn/2bd4897f130c46a990be1698bd2f28d9.png)

![](https://img-blog.csdnimg.cn/ba62c90665be4ace9db5ceb65ad5a13e.png)

###  2.8 Further Experiments—进一步的实验

### 翻译

**进一步的实验**我们训练 YOLOv2 对 VOC 2012 进行检测。表 4 显示了 YOLOv2 与其他最先进的检测系统的性能比较。YOLOv2 实现了 73.4 mAP，同时运行速度远远超过比较的方法。我们还对 COCO 进行了训练，并在表 5 中与其他方法进行了比较。在 VOC 指标（IOU = 0.5）上，YOLOv2 得到 44.0 mAP，与 SSD 和 Faster R-CNN 相当。

### 精读

作者在 VOC2012 上对 YOLOv2 进行训练，下图是和其他方法的对比。YOLOv2 精度达到了 73.4%，并且速度更快。

![](https://img-blog.csdnimg.cn/0876d72b61a64fb583763bbfd0bd2ff5.png)

同时 YOLOV2 也在 COCO 上做了测试（IOU=0.5），也和 Faster R-CNN、SSD 作了成绩对比。总的来说，比上不足，比下有余。

![](https://img-blog.csdnimg.cn/dd9fd61e26574095b51edfdaa321d56c.png)

三、Faster—更快
-----------

### 翻译

我们希望检测是准确的，但我们也希望它是快速的。大多数检测的应用，如机器人或自动驾驶汽车，都依赖于低延迟的预测。为了最大限度地提高性能，我们在设计 YOLOv2 时从头到尾都是快速的。

大多数检测框架依靠 VGG-16 作为基础特征提取器 [17]。VGG-16 是一个强大的、准确的分类网络，但它是不必要的复杂。VGG-16 的卷积层需要 306.9 亿次浮点运算来处理一张 224×224 分辨率的图像。

YOLO 框架使用一个基于 Googlenet 架构的定制网络 [19]。这个网络比 VGG-16 更快，一个前向通道只用了 85.2 亿次运算。然而，它的准确性比 VGG16 略差。对于 224×224 的单张图像，前 5 名的准确率，YOLO 在 ImageNet 上的自定义模型精度为 88.0%，而 VGG-16 为 90.0%。

### 精读

这一段开头先批评一波 VGG（[经典神经网络论文超详细解读（二）——VGGNet 学习笔记（翻译＋精读）](https://blog.csdn.net/weixin_43334693/article/details/128148803?spm=1001.2014.3001.5501 "经典神经网络论文超详细解读（二）——VGGNet学习笔记（翻译＋精读）")），说 VGG 慢的不行，所以 YOLOv1 用的 GoogLeNet，也就是 Inceptionv1（[经典神经网络论文超详细解读（三）——GoogLeNet InceptionV1 学习笔记（翻译＋精读 + 代码复现](https://blog.csdn.net/weixin_43334693/article/details/128267380?spm=1001.2014.3001.5501 "经典神经网络论文超详细解读（三）——GoogLeNet InceptionV1学习笔记（翻译＋精读+代码复现")）。速度很快，但是对比 VGG 精度稍微有所下降

**通常目标检测框架：** 大多数检测框架依赖于 VGG-16 作为基本的特征提取器。VGG-16 是一个强大、精确的分类网络，但是它计算复杂。

**YOLO 框架：** 使用基于 GoogLeNet 架构的自定义网络。虽说整体 mAP 表现较 VGG-16 差一些，但是却换来更快速、更少的预测运算。

**YOLOv2 框架：** 使用的是一个全新的架构: Darknet-19

### 3.1 Darknet-19

### 翻译

**Darknet-19** 我们提出一个新的分类模型，作为 YOLOv2 的基础。我们的模型建立在先前的网络设计工作以及该领域的常识之上。与 VGG 模型类似，我们主要使用 3×3 的过滤器，并在每个池化步骤后将通道的数量增加一倍 [17]。按照网络中的网络（NIN）的工作，我们使用全局平均池来进行预测，以及使用 1×1 滤波器来压缩 3×3 卷积之间的特征表示 [9]。我们使用批量归一化来稳定训练，加速收敛，并使模型正规化 [7]。

我们的最终模型，称为 Darknet-19，有 19 个卷积层和 5 个 maxpooling 层。完整的描述见表 6。Darknet-19 只需要 55.8 亿次操作来处理一幅图像，却在 ImageNet 上达到了 72.9% 的最高准确率和 91.2% 的 top-5 准确率。

### 精读

#### **Darknet-19 介绍**

 一个新的分类模型作为 YOLOv2 的基础框架。与 VGG 模型类似，主要使用 3×3 的卷积，并在每个池化步骤后加倍通道数。使用全局平均池进行预测，并使用 1×1 卷积压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用 BN 层以加快模型收敛同时防止过拟合。最后用 average pooling 层代替全连接层进行预测。

**Darknet-19 细节：** 有 19 个卷积层和 5 个 maxpooling 层。（v1 的 GooLeNet 是 4 个卷积层和 2 个全连接层）

**结构如下：**（这是分类的模型，不是目标检测的模型）

![](https://img-blog.csdnimg.cn/414ea2c55121463c858646fc3514c86a.png)

采用 YOLOv2，模型的 mAP 值没有显著提升，但计算量减少了。

> Q：为什么去掉全连接层了呢？
> 
> 因为全连接层容易过拟合，训练慢。（参数太多）如下图，YOLOv1 中通过全连接层将 7×7×1024 的特征图变换为 7×7×30 的特征图。但是这种变换完全可以通过一个 3×3 的卷积核做到，从而节省参数。
> 
> ![](https://img-blog.csdnimg.cn/ef6724b75b2e45699e48d70abbe9189c.png)

### 3.2 Training for classification—分类的训练

### 翻译

**分类的训练** 我们使用随机梯度下降法在标准的 ImageNet 1000 类分类数据集上训练网络 160 次，使用 Darknet 神经网络框架 [13]，起始学习率为 0.1，多项式速率衰减为 4 次方，权重衰减为 0.0005，动量为 0.9。在训练过程中，我们使用标准的数据增强技巧，包括随机作物、旋转、色调、饱和度和曝光度的转变。

如上所述，在对 224×224 的图像进行初始训练后，我们在更大的尺寸（448）上对我们的网络进行微调。在这种微调中，我们用上述参数进行训练，但只用了 10 个 epoch，并以 10-3 的学习率开始。在这个更高的分辨率下，我们的网络达到了 76.5% 的最高准确率和 93.3% 的 Top-5 准确率。

### 精读

#### 参数设置

**（1）训练数据集：** 标准 ImageNet 1000 类分类数据集

**（2）训练参数：** 对网络进行 160 个 epochs 的训练，使用初始学习率为 0.1 随机梯度下降法、4 的多项式率衰减法、0.0005 的权值衰减法和 0.9 的动量衰减法

**（3）模型：** 使用的是 Darknet 神经网络框架。

**（4）数据增强：** 在训练中使用标准的数据增强技巧，包括随机的裁剪、旋转、色相、饱和度和曝光变化。

如上所述，在最初的 224×224 图像训练之后，然后放到 448 × 448 上微调，但只训练约 10 个周期。在这个高分辨率下，网络达到很高精度。微调时，10epoch，初始 lr0.001。

**结果：** 高分辨率下训练的分类网络在 top-1 准确率 76.5%，top-5 准确率 93.3%。

### 3.3 Training for detection—检测的训练

### 翻译

**检测的训练** 我们对这个网络进行了修改，去掉了最后一个卷积层，而是增加了三个 3×3 的卷积层，每个卷积层有 1024 个过滤器，然后是最后一个 1×1 的卷积层，输出的数量是我们检测所需的。对于 VOC，我们预测 5 个框的 5 个坐标，每个框有 20 个类别，所以有 125 个过滤器。我们还从最后的 3×3×512 层向第二个卷积层添加了一个直通层，以便我们的模型可以使用细粒度的特征。

我们用 10-3 的起始学习率训练网络 160 个 epoch，在 60 和 90 个 epoch 时除以 10。我们使用 0.0005 的权重衰减和 0.9 的动量。我们使用与 YOLO 和 SSD 类似的数据增强，包括随机裁剪、颜色转换等。我们在 COCO 和 VOC 上使用同样的训练策略。

### 精读

#### **网络微调**

*   移除最后一个卷积层、global avgpooling 层和 softmax
*   增加 3 个 3x3x1024 的卷积层
*   增加 passthrough 层
*   增加一个 1×1 个卷积层作为网络输出层。输出的 channel 数为 num_ anchors×(5+num_ calsses)（num_anchors 在文中为 5，num _classes=20 是类别个数，5 是坐标值和置信度）

#### 细节

（1）**网络最后一层即 1X1 卷积层卷积核个数同网络输出维度相同：** 对于 VOC，预测 5 个边界框，每个边界框有 5 个坐标，每个边界框有 20 个类，所以最后一个 1×1 卷积层有 125 个卷积核。

（2）**passthrough 层：** 倒数第二个 3X3 卷积到最后一个 3X3 卷积层增加 passthrough 层。模型可以使用细粒度的特征。

（3）**训练参数：** 10−3 的起始学习率对网络进行 160 个周期的训练，并在 60 和 90 个周期时将其除以 10。使用重量衰减为 0.0005，动量为 0.9。

#### YOLOv2 的训练

（1）在 ImageNet 训练 Draknet-19，模型输入为 224×224，共 160 个 epochs

（2）将网络的输入调整为 448×448, 继续在 ImageNet 数据集上 finetune 分类模型，训练 10 个 epochs。参数除了 epoch 和 learning rate 改变外，其他都没变，这里 learning rate 改为 0.001。

（3）修改 Darknet-16 分类模型为检测模型（看上面的网络微调部分），并在监测数据集上继续 finetune 模型

![](https://img-blog.csdnimg.cn/69e7c509d9a0411a8e5600f0b5853fde.png)

四、Stronger—更强（YOLO9000 部分）
--------------------------

### 翻译

我们提出了一种对分类和检测数据进行联合训练的机制。我们的方法使用标记为检测的图像来学习特定的检测信息，如边界框坐标预测和目标类，以及如何对普通目标进行分类。它使用只有类别标签的图像来扩大它可以检测的类别的数量。

在训练过程中，我们混合了来自检测和分类数据集的图像。当我们的网络看到被标记为检测的图像时，我们可以根据完整的 YOLOv2 损失函数进行反向传播。当它看到一个分类图像时，我们只从架构的分类特定部分反向传播损失。

这种方法带来了一些挑战。检测数据集只有常见的物体和一般的标签，如 "狗" 或 “船”。分类数据集有更广泛和更深入的标签范围。ImageNet 有一百多个狗的品种，包括 “诺福克梗”、"约克夏梗" 和 “贝灵顿梗”。如果我们想在这两个数据集上进行训练，我们需要一个连贯的方法来合并这些标签。

大多数分类方法在所有可能的类别中使用 softmax 层来计算最终的概率分布。使用 softmax 时，假定这些类别是相互排斥的。这给合并数据集带来了问题，例如，你不会想用这个模型来合并 ImageNet 和 COCO，因为 "诺福克梗" 和 "狗" 这两个类别并不相互排斥。

我们可以使用一个多标签模型来结合数据集，而这个模型并不假定相互排斥。这种方法忽略了我们所知道的关于数据的所有结构，例如，所有的 COCO 类都是互斥的。

### 精读

#### YOLOv2 和 YOLO9000 的关系

YOLOv2 和 YOLO9000 算法在 2017 年 CVPR 上被提出，重点解决 YOLOv1 召回率和定位精度方面的误差。

**YOLOv2：** 是在 YOLOv1 的基础上改进得到，改进之处主要有：Batch Normalization (批量归一化)、High Resolution Classfier(高分辨率的分类器)、Convolutional With Anchor Boxes (带锚框的卷积)、Dimension Clusters (维度聚类)、Direct location prediction (直接位置预测)、Fine-Grained Feature (细粒度特性)、Multi-Scale Training (多尺度训练)，它的特点是 “更好，更快，更强”。

**YOLO9000：** 的主要检测网络也是 YOLO v2，同时使用 WordTree 来混合来自不同的资源的训练数据，并使用联合优化技术同时在 ImageNet 和 COCO 数据集上进行训练，目的是利用数量较大的分类数据集来帮助训练检测模型，因此，YOLO9000 的网络结构允许实时地检测超过 9000 种物体分类，进一步缩小了检测数据集与分类数据集之间的大小代沟。

#### 方法

联合 coco 目标检测数据集和 imagenet 分类数据集。

*   输入的若为目标检测标签的，则在模型中反向传播目标检测的损失函数。
*   输入的若为分类标签的，则反向传播分类的损失函数

#### 问题

*   coco 的数据集标签分类的比较粗，比如狗，猫，而 imagenet 分类则比较细化，比如二哈狗，金毛狗。
*   这时候如果用 softmax 进行最后的分类，则会产生问题，因为 softmax 输出最大概率的那个分类，各种分类之间彼此互斥，若狗，二哈狗，金毛狗在一起的话就会出问题。
*   所以要联合训练，必须让标签有一定程度上的一致性。

### 4.1 Hierarchical classification—分层分类

### 翻译

**分层分类** ImageNet 的标签是从 WordNet 中提取的，WordNet 是一个语言数据库，用于构造概念和它们之间的关系 [12]。在 WordNet 中，"Norfolk terrier" 和 "Yorkshire terrier" 都是 "terrier" 的外来语，而 "terrier" 是 "猎狗" 的一种，是 "狗" 的一种，是 "犬类" 的一种等等。大多数分类方法都假定标签有一个平面结构，然而对于结合数据集来说，结构正是我们所需要的。

WordNet 的结构是一个有向图，而不是一棵树，因为语言是复杂的。例如，"狗" 既是 "犬类" 的一种类型，也是 "家畜" 的一种类型，它们都是 WordNet 中的主题词。我们没有使用完整的图结构，而是通过从 ImageNet 中的概念建立一棵分层的树来简化这个问题。

为了建立这棵树，我们检查了 ImageNet 中的视觉名词，并查看了它们通过 WordNet 图到根节点的路径，在这个例子中是 “物理对象”。许多同义词在图中只有一条路径，因此我们首先将所有这些路径添加到我们的树上。然后，我们反复检查我们剩下的概念，并添加路径，使树的增长尽可能少。因此，如果一个概念有两条通往根的路径，其中一条路径会给我们的树增加三条边，而另一条只增加一条边，我们就选择较短的路径。

最后的结果是 WordTree，一个视觉概念的分层模型。为了用 WordTree 进行分类，我们在每个节点上预测条件概率，即在给定的同义词中，每个同义词的概率。如果我们想计算一个特定节点的绝对概率，我们只需沿着树的路径到根节点，然后乘以条件概率。

为了分类的目的，我们假设该图像包含一个物体。Pr(物理对象) = 1。

为了验证这种方法，我们在使用 1000 类 ImageNet 建立的 WordTree 上训练 Darknet-19 模型。为了建立 WordTree1k，我们加入了所有的中间节点，将标签空间从 1000 扩大到 1369。在训练过程中，我们在树上传播基础事实标签，这样，如果一张图片被标记为 “诺福克梗”，它也会被标记为 "狗" 和 “哺乳动物”，等等。为了计算条件概率，我们的模型预测了一个由 1369 个值组成的向量，我们计算了所有作为同一概念的假名的系统集的 softmax，见图 5。

![](https://img-blog.csdnimg.cn/42eb77ffcc64491f8bf26995f02922ae.png)

使用与之前相同的训练参数，我们的分层式 Darknet-19 达到了 71.9% 的 top-1 准确率和 90.4% 的 top-5 准确率。尽管增加了 369 个额外的概念，并让我们的网络预测树状结构，但我们的准确率只下降了一点。以这种方式进行分类也有一些好处。在新的或未知的对象类别上，性能会优雅地下降。例如，如果网络看到一张狗的照片，但不确定它是什么类型的狗，它仍然会以高置信度预测 “狗”，但在假名中分布的置信度会降低。

这种表述也适用于检测。现在，我们不是假设每张图片都有一个物体，而是使用 YOLOv2 的物体性预测器来给我们提供 Pr（物理物体）的值。检测器会预测出一个边界框和概率树。我们向下遍历这棵树，在每一个分叉处采取最高的置信度路径，直到我们达到某个阈值，我们就可以预测那个物体类别。

### 精读

*   ImageNet 的标签是从 WordNet 中提取的，WordNet 是一个语言数据库，用于构造概念和它们之间的关系。
*   WordNet 的结构是一个有向图，而不是一棵树，因为语言是复杂的。
*   作者们并不采用整个 WordNet 的图结构，而是从中抽取其视觉名词重新制作一个树状结构。
*   在 WordTree 结构上进行操作，需要预测的是每一个节点相对于父节点的条件概率，要计算某个几点的绝对概率 或者说联合概率，就直接从他乘到根节点。

![](https://img-blog.csdnimg.cn/2144e4b95426452eb90c0f85e22d1ac6.png)

### 4.2 Dataset combination with WordTree—用 WordTree 组合数据集

### 翻译

**用 WordTree 组合数据集。**我们可以使用 WordTree 以合理的方式将多个数据集组合在一起。我们只需将数据集中的类别映射到树上的同位素。图 6 显示了一个使用 WordTree 来结合 ImageNet 和 COCO 的标签的例子。WordNet 是非常多样化的，所以我们可以将这种技术用于大多数数据集。

### 精读

原始正常的数据集中数据结构是 WordNet(有向图)。作者改造成了 WordTree(树)。

**WordTree 的生成方式如下：**

*   遍历 Imagenet 的 label，然后在 WordNet 中寻找该 label 到根节点 (指向一个物理对象) 的路径；
*   如果路径只有一条，那么就将该路径直接加入到分层树结构中；
*   否则，从剩余的路径中选择一条最短路径，加入到分层树。

![](https://img-blog.csdnimg.cn/02f872dc2a5242339fdacb7a6190f03e.png)

混合后的数据集形成一个有 9418 类的 WordTree。生成的 WordTree 模型如下图所示。另外考虑到 COCO 数据集相对于 ImageNet 数据集数据量太少了，为了平衡两个数据集，作者进一步对 COCO 数据集过采样，使 COCO 数据集与 ImageNet 数据集的数据量比例接近 1：4。

### 4.3 Joint classification and detection—联合分类和检测

### 翻译

**联合分类和检测** 现在我们可以使用 WordTree 结合数据集，我们可以训练分类和检测的联合模型。我们想训练一个极大规模的检测器，所以我们使用 COCO 检测数据集和 ImageNet 完整版本中的前 9000 个类来创建我们的联合数据集。我们还需要评估我们的方法，所以我们加入了 ImageNet 检测挑战中尚未包括的任何类别。这个数据集的相应 WordTree 有 9418 个类。ImageNet 是一个更大的数据集，所以我们通过对 COCO 的过度采样来平衡数据集，使 ImageNet 只比它大 4:1。

使用这个数据集，我们训练 YOLO9000。我们使用基本的 YOLOv2 架构，但只有 3 个先验因素，而不是 5 个，以限制输出大小。当我们的网络看到一个检测图像时，我们像平常一样反向传播损失。对于分类损失，我们只在标签的相应级别或以上反向传播损失。例如，如果标签是 “狗”，我们不给树上更远的预测分配任何错误，"德国牧羊犬" 与 “金毛猎犬”，因为我们没有这些信息。

当它看到一个分类图像时，我们只反向传播分类损失。要做到这一点，我们只需找到预测该类的最高概率的边界框，并计算其预测树上的损失。我们还假设预测框与地面真实标签至少有 0.3 IOU 的重叠，我们根据这一假设反向传播对象性损失。

通过这种联合训练，YOLO9000 学会了使用 COCO 中的检测数据来寻找图像中的物体，并学会了使用 ImageNet 中的数据对这些物体进行分类。

我们在 ImageNet 检测任务上评估了 YOLO9000。ImageNet 的检测任务与 COCO 共享 44 个对象类别，这意味着 YOLO9000 只看到了大多数测试图像的分类数据，而不是检测数据。YOLO9000 总体上得到了 19.7 的 mAP，在它从未见过任何标记的检测数据的 156 个不相干的对象类别上得到了 16.0 的 mAP。这个 mAP 比 DPM 取得的结果要高，但是 YOLO9000 是在不同的数据集上训练的，只有部分监督 [4]。它还同时检测了 9000 个其他物体类别，而且都是实时的。

当我们分析 YOLO9000 在 ImageNet 上的表现时，我们看到它能很好地学习新的动物物种，但在学习服装和设备等类别时却很困难。新的动物更容易学习，因为对象性预测可以很好地从 COCO 中的动物中概括出来。相反，COCO 没有任何类型的衣服的边界框标签，只有人的标签，所以 YOLO9000 在为 "太阳镜" 或 "游泳裤" 等类别建模时很吃力。

### 精读

#### YOLO9000 是怎样进行联合训练的？

YOLO9000 采用 YOLO v2 的结构，**Anchorbox 由原来的 5 调整到 3**，对每个 Anchorbox 预测其对应的边界框的位置信息 x , y , w , h 和置信度以及所包含的物体分别属于 9418 类的概率，所以每个 Anchorbox 需要预测 4+1+9418=9423 个值。每个网格需要预测 3×9423=28269 个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的 YOLO v2 loss 进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的 loss 进行反向传播。

#### YOLO 9000 是怎么预测的？

WordTree 中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次 softmax 处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在 WordTree 上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。**预测时， YOLO v2 得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。**

### **五、Conclusion—结论**

### **翻译**

我们介绍了 YOLOv2 和 YOLO9000，实时检测系统。YOLOv2 是最先进的，在各种检测数据集上比其他检测系统快。此外，它可以在各种图像尺寸下运行，在速度和准确性之间提供平稳的权衡。

YOLO9000 是一个实时框架，通过联合优化检测和分类来检测 9000 多个物体类别。我们使用 WordTree 来结合各种来源的数据和我们的联合优化技术，在 ImageNet 和 COCO 上同时训练。YOLO9000 是朝着缩小检测和分类之间的数据集大小差距迈出的有力一步。

我们的许多技术可以在目标检测之外进行推广。我们对 ImageNet 的 WordTree 表示为图像分类提供了一个更丰富、更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将是有用的。像多尺度训练这样的训练技术可以在各种视觉任务中提供好处。

对于未来的工作，我们希望将类似的技术用于弱监督的图像分割。我们还计划在训练过程中使用更强大的匹配策略为分类数据分配弱标签来提高我们的检测结果。计算机视觉有着得天独厚的大量标记数据。我们将继续寻找方法，将不同来源和结构的数据结合起来，为视觉世界建立更强大的模型。

### **精读**

**YOLOv2** 是最先进的，在各种检测数据集上比其他检测系统更快。此外，它可以在各种图像大小下运行，以在速度和精度之间提供平滑的折中。

**对比 yolov1 所作出的改进：**

*   加了 BN（卷积后，激活函数前）;
*   加了高分辨率分类器; 加了 anchor(聚类得到个数, 1 个 gird cell 生成 5 个 anchor); 限制预测框；
*   加入细粒度特征 (类似于 concat 的残差) 加入对尺度训练改进骨干网络 (GoogleNet 变 darknet-19) 通过 WordTree 将不同数据集结合联合训练。
*   用一种新颖的方法扩充了数据集。

**YOLO9000** 是一个实时框架，通过联合优化检测和分类，可检测 9000 多个对象类别。我们使用 WordTree 合并来自不同来源的数据，并使用我们的联合优化技术在 ImageNet 和 CoCo 上同时进行训练。

**WordTree** 的概念可以让分类标注提供更大的运用空间，并且可以利用来进行弱监督学习，也可以利用这样的概念结合各种不同任务的资料集，对于分类有很大的助益。

本篇到这就结束了，我们 YOLOv3 见~