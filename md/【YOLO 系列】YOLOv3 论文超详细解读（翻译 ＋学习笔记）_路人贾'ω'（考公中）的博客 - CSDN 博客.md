> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [blog.csdn.net](https://blog.csdn.net/weixin_43334693/article/details/129143961?spm=1001.2014.3001.5501)

![](https://img-blog.csdnimg.cn/ce594cde0fce414da99a1b3b11c46d54.gif)
---------------------------------------------------------------------

前言
--

[YOLOv3](https://so.csdn.net/so/search?q=YOLOv3&spm=1001.2101.3001.7020)（《Yolov3:An incremental improvement》）是 Joseph Redmon 大佬关于 YOLO 系列的最后一篇，由于他反对将 YOLO 用于军事和隐私窥探，2020 年 2 月宣布停止更新 YOLO。

![](https://img-blog.csdnimg.cn/0077b186b18946d7a8443049a7b73185.png)

 YOLOv3 在 YOLOv2 的基础上改良了网络的主干，利用多尺度特征图进行检测，改进了多个独立的 Logistic regression 分类器来取代 softmax 来预测类别分类。这篇论文（或许称它为学术报告更合适）相当有趣，是我目前读过最欢乐的一篇了。十分建议大家读读原文，感受任性大佬的幽默感。

**学习资料：** 

论文原文：[YOLOv3.pdf (pjreddie.com)](https://pjreddie.com/media/files/papers/YOLOv3.pdf "YOLOv3.pdf (pjreddie.com)")

项目源码：[mirrors / ultralytics / yolov3 · GitCode](https://gitcode.net/mirrors/ultralytics/yolov3?utm_source=csdn_github_accelerator "mirrors / ultralytics / yolov3 · GitCode") 

项目主页：[YOLO: Real-Time Object Detection (pjreddie.com)](https://pjreddie.com/darknet/yolo/ "YOLO: Real-Time Object Detection (pjreddie.com)")

**YOLO 系列前情回顾：** 

[【YOLO 系列】YOLOv2 论文超详细解读（翻译 ＋学习笔记）](https://blog.csdn.net/weixin_43334693/article/details/129087464?spm=1001.2014.3001.5501 "【YOLO系列】YOLOv2论文超详细解读（翻译 ＋学习笔记）")

[【YOLO 系列】YOLOv1 论文超详细解读（翻译 ＋学习笔记）](https://blog.csdn.net/weixin_43334693/article/details/129011644?spm=1001.2014.3001.5501 "【YOLO系列】YOLOv1论文超详细解读（翻译 ＋学习笔记）")

**目录**
------

[前言](#%E5%89%8D%E8%A8%80)

[Abstract—摘要](#t2)

[一、Introduction—（随性的）引言](#t3)

[二、The Deal—改进的细节](#t4)

[2.1 Bounding Box Prediction—边界框预测](#t5)

[2.2 Class Prediction—类预测（单标签分类改进为多标签分类）](#t6)

 [2.3 Predictions Across Scales—跨尺度预测](#t7)

[2.4 Feature Extractor—特征提取](#t8)

[2.5 Training—训练](#t9)

[三、How We Do—我们怎样做](#t10)

[四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法](#t11)

[五、 What This All Means—这一切意味着什么？](#t12)

[总结](#t13) 

**Abstract—摘要**
---------------

### **翻译**

我们提出了对 [YOLO](https://so.csdn.net/so/search?q=YOLO&spm=1001.2101.3001.7020) 的一些更新! 我们做了一些小的设计改动，使其变得更好。我们还训练了这个新的网络，这个网络非常棒。它比上次大了一点，但更准确。不过它仍然很快，不用担心。在 320×320 的情况下，YOLOv3 在 28.2mAP 的情况下运行 22 毫秒，与 SSD 一样准确，但速度快三倍。当我们看一下旧的 0.5 IOU mAP 检测指标时，YOLOv3 是相当好的。在 Titan X 上，它在 51 毫秒内实现了 57.9 个 AP50，而 RetinaNet 在 198 毫秒内实现了 57.5 个 AP50，性能相似但快 3.8 倍。像往常一样，所有的代码都在网上，https://pjreddie.com/yolo/。

### **精读**

**对比 YOLOv2：** 做了一些小改进，然后训练了一个更深的模型，准确度有所提升，并且依旧很快。

**对比 SSD：** 在 320×320 的情况下，YOLOv3 在 28.2mAP 的情况下运行 22 毫秒，与 SSD 一样准确，但速度快三倍。

**对比 RetinaNet：** 在 Titan X 上，它在 51 毫秒内实现了 57.9 个 AP50，而 RetinaNet 在 198 毫秒内实现了 57.5 个 AP50，性能相似但快 3.8 倍。

这是可爱的作者直接拿了 RetinaNet 的图，然后加上了自己的 YOLO 曲线，可以看到 YOLO 的线都已经弄到坐标轴外面去了，这是故意的，用这种方法凸显自己模型更快（~还有个原因是因为懒~）。

 ![](https://img-blog.csdnimg.cn/eb927c3c8cc647f5a2e248362237de8b.png)

**一、Introduction—（随性的）引言**
--------------------------

### **翻译**

有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。我有一点去年留下的动力 [12] [1]；我设法对 YOLO 做了一些改进。但是，说实话，没有什么超级有趣的东西，只是做了一些小改动，让它变得更好。我还对其他人的研究提供了一些帮助。

实际上，这也是我们今天来到这里的原因。我们有一个可以上镜的最后期限 [4]，我们需要引用我对 YOLO 的一些随机更新，但我们没有来源。所以准备好接受技术报告吧!

技术报告的好处是，它们不需要介绍，你们都知道我们为什么在这里。因此，这个介绍的结尾将为本文的其余部分做一个标志。首先，我们将告诉你 YOLOv3 是什么情况。然后我们会告诉你我们是如何做的。我们也会告诉你一些我们尝试过但没有成功的事情。最后，我们将思考这一切意味着什么。

### **精读**

“有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。”——最开始读时我一度以为这是网友的恶搞翻译，直到后来自己看了原文。。。不得不服，果真是大佬才敢那么任性吧！

很别具一格，没啥实际内容，大家直接往后翻吧 hh。就是说这篇论文实际上算是一篇学术报告，是为了给他们即将出现的新论文做引用的。

**二、The Deal—改进的细节**
--------------------

### 2.1 Bounding Box Prediction—边界框预测

### 翻译

按照 YOLO9000，我们的系统使用维度集群作为锚定框来预测边界框 [15]。该网络为每个边界框预测 4 个坐标，tx, ty, tw, th。如果单元格从图像的左上角偏移（cx，cy），并且边界框先验具有宽度和高度 pw，ph，那么预测对应于：

![](https://latex.csdn.net/eq?b_%7Bx%7D%3D%5Csigma%20%28t_%7Bx%7D%29&plus;c_%7Bx%7D)

![](https://latex.csdn.net/eq?b_%7By%7D%3D%5Csigma%20%28t_%7By%7D%29&plus;c_%7By%7D)

![](https://latex.csdn.net/eq?b_%7Bw%7D%3Dp_%7Bw%7De%5E%7Bt_%7Bw%7D%7D)

![](https://latex.csdn.net/eq?b_%7Bh%7D%3Dp_%7Bh%7De%5E%7Bt_%7Bh%7D%7D)

![](https://latex.csdn.net/eq?Pr%28object%29*IOU%28b%2Cobject%29%3D%5Csigma%20%28t_%7B0%7D%29)

在训练期间，我们使用平方误差损失之和。如果某个真实框的坐标是ˆt*，我们的梯度就是真实框坐标值（从真实框中计算）减去我们的预测坐标值：ˆt* - t*。这个真实框坐标值可以通过倒置上述方程轻松计算出来。 

![](https://img-blog.csdnimg.cn/00a220d16c8f4eb1a677c9584b2e1799.png)

 YOLOv3 使用逻辑回归法为每个边界框预测一个目标分数。如果边界框先验比其他边界框先验更多地与目标真实框重叠，则该分数应为 1。如果先验边界框不是最好的，但与真实框目标的重叠程度超过了某个阈值，我们就忽略这个预测，遵循 [17]。我们使用 0.5 的阈值。与[17] 不同的是，我们的系统只为每个真实框对象分配一个先验边界框。如果没有为一个真实框对象分配一个先验边界框，它不会对坐标或类别的预测产生任何损失，只有目标置信度。

### 精读

#### 与 YOLOv2 相同之处

使用 dimension clusters 来找到先验 anchor boxes，然后通过 anchor boxes 来预测边界框。

#### YOLOv3 的改进

在 YOLOv3 中，利用逻辑回归来预测每个边界框的客观性分数 (object score)，也就是 YOLOv1 论文中说的 confidence :

 **●  正样本：** 如果当前预测的包围框比之前其他的任何包围框更好的与 ground truth 对象重合，那它的置信度就是 1。

 **●  忽略样本：** 如果当前预测的包围框不是最好的，但它和 ground truth 对象重合了一定的阈值（这里是 0.5）以上，神经网络会忽略这个预测。

 **●  负样本:** 若 bounding box 没有与任一 ground truth 对象对应，那它的置信度就是 0

> Q1：为什么 YOLOv3 要将正样本 confidence score 设置为 1?
> 
> 置信度意味着该预测框是或者不是一个真实物体，是一个二分类，所以标签是 1、0 更加合理。并且在学习小物体时，有很大程度会影响 IOU。如果像 YOLOv1 使用 bounding box 与 ground truth 对象的 IOU 作为 confidence，那么 confidence score 始终很小，无法有效学习，导致检测的 Recall 不高。

> Q2：为什么存在忽略样本?
> 
> 由于 YOLOV3 采用了多尺度的特征图进行检测，而不同尺度的特征图之间会有重合检测的部分。例如检测一个物体时，在训练时它被分配到的检测框是第一个特征图的第三个 bounding box，IOU 为 0.98，此时恰好第二个特征图的第一个 bounding box 与该 ground truth 对象的 IOU 为 0.95，也检测到了该 ground truth 对象，如果此时给其 confidence score 强行打 0，网络学习的效果会不理想。

与 Faster-RCNN 不同，YOLOv3 仅对每一个真实物件分配一个 anchor box，若没有分配到 anchor box 的真实物件，便不会有坐标误差，仅会具有 object score 误差。

### 2.2 Class Prediction—类预测（单标签分类改进为多标签分类）

### 翻译

每个框都使用多标签分类法预测边界框可能包含的类别。我们不使用 softmax，因为我们发现它对于良好的性能是不必要的，相反，我们只是使用独立的逻辑分类器。在训练过程中，我们使用二元交叉熵损失来进行分类预测。

当我们转向更复杂的领域，如开放图像数据集 [7] 时，这种提法会有所帮助。在这个数据集中，有许多重叠的标签（即女人和人）。

使用 softmax 的假设是，每个框都有一个确切的类别，但情况往往并非如此。多标签方法可以更好地模拟数据。

### 精读

#### 原因

每个框使用多标签分类预测边界框可能包含的类

#### YOLOv3 使用的方法

（1）YOLOv3 使用的是 **logistic 分类器**，而不是之前使用的 softmax。

（2）在 YOLOv3 的训练中，便使用了 **Binary Cross Entropy (BCE, 二元交叉熵)** 来进行类别预测。

> Q：softmax 被替代的原因？
> 
> （1）softmax 只适用于单目标多分类 (甚至类别是互斥的假设)，但目标检测任务中可能一个物体有多个标签。(属于多个类并且类别之间有相互关系)，比如 Person 和 Women。
> 
> （2）logistic 激活函数来完成，这样就能预测每一个类别是 or 不是。

###  2.3 Predictions Across Scales—跨尺度预测

### 翻译

YOLOv3 在三个不同的尺度上对框进行预测。我们的系统使用类似于特征金字塔网络 [8] 的概念从这些尺度上提取特征。从我们的基础特征提取器中，我们添加了几个卷积层。最后一个卷积层预测一个 3 维张量，编码边界框、对象性和类别预测。在我们与 COCO[10]的实验中，我们在每个尺度上预测 3 个框，所以张量是 N×N×[3∗(4+1+80)]，用于 4 个边界盒的偏移，1 个对象性预测，和 80 个类别预测。

接下来，我们从前两层中提取特征图，并对其进行 2 倍的上采样。我们还从网络中的早期特征图中取出一个特征图，用连接法将其与我们的上采样特征合并。这种方法使我们能够从上采样的特征中获得更有意义的语义信息，并从早期的特征图中获得更精细的信息。然后，我们再增加几个卷积层来处理这个合并的特征图，最终预测出一个类似的张量，尽管现在的张量是原来的两倍。

我们再进行一次同样的设计，以预测最终规模的框。因此，我们对第三个尺度的预测得益于所有先前的计算以及网络早期的细化特征。

我们仍然使用 k-means 聚类法来确定我们的边界框预设。我们只是任意地选择了 9 个聚类和 3 个尺度，然后在各个尺度上均匀地划分聚类。在 COCO 数据集上，这 9 个聚类是。(10 × 13), (16 × 30), (33 × 23), (30 × 61), (62 × 45), (59 × 119), (116 × 90), (156 × 198), (373 × 326)。

### 精读

#### YOLOv3 的改进

**灵感来源：** YOLOv3 借鉴了 FPN 的方法，采用多尺度的特征图对不同大小的物体进行检测，以提升小物体的预测能力。

![](https://img-blog.csdnimg.cn/2369106f63ec4cb7b78ebf6dbe799d00.png)

**（1）YOLOv3 采用了 3 个不同尺度的特征图（三个不同卷积层提取的特征）**

YOLOv3 通过下采样 32 倍、16 倍和 8 倍得到 3 个不同尺度的特征图。

![](https://img-blog.csdnimg.cn/c573e99db4bf43af938037dcc8caf46e.png)

例如输入 416X416 的图像，则会得到 **13X13** (416/32)，**26X26**(416/16) 以及 **52X52**(416/8) 这 3 个尺度的特征图。

![](https://img-blog.csdnimg.cn/2b46b8d08753434aba1ce16e7024f909.png)

**（2）YOLOv3 每个尺度的特征图上使用 3 个 anchor box。**

使用 dimension clusters 得到 9 个聚类中心（anchor boxes），并将这些 anchor boxes 划分到 3 个尺度特征图上，尺度更大的特征图使用更小的先验框。

![](https://img-blog.csdnimg.cn/05d5a56bfb384bf6adafe5fef6447b6c.png)**（3）YOLOv3 对每个尺度下的特征图都进行边界框的预测。**

每种尺度的特征图上可以得到 **N × N × [3 ∗ (4 + 1 + 80)]** 的结果（分别是 N x N 个 gird cell ，3 种尺度的 anchor boxes，4 个边界框偏移值、1 个目标预测置信度以及 80 种类别的预测概率。）

![](https://img-blog.csdnimg.cn/5b6ab09c73cd4ec884e5e9c05e086634.png)

该方法允许从上采样的特征中获取更有意义的语义信息，从早期的特征图中获取更细粒度的信息。

#### 不同尺度下的预测方法

![](https://img-blog.csdnimg.cn/0916c5a477a74d419b48acc64b447f77.png)

**（1）第一种尺度：**

**特征图：** 对原图下采样 32x 得到 (13 x 13) 特征图

**预测：** 在上述特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。——图中第 3 个红色部分

**最终输出：** [13, 13, 255]

**（2）第二种尺度：**

**特征图：** 来源于两种计算

    1. 对原图下采样 16x 得到 (26 x 26) 特征图

    2. 对第一种尺度得到的 (13 x 13) 特征图进行上采样，得到 (26 x 26) 特征图。

两种计算得到的（26 x 26）特征图通过连接合并在一起。

**预测：** 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度一输出张量大小的两倍。——图中第 2 个红色部分

**最终输出：** [26, 26, 255]

**（3）第三种尺度：**

**特征图：** 来源于两种计算

    1. 对原图下采样 8x 得到 (52 x 52) 特征图

    2. 对第二种尺度得到的 (26 x 26) 特征图进行上采样，得到 (52 x 52) 特征图。

两种方式得到的（52 x 52）的特征图通过连接合并在一起。

**预测：** 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度二输出的两倍 ---- 图中第 1 个红色部分。

对第三尺度的预测受益于所有的先验计算以及网络早期的细粒度特性。

**最终输出：** [52, 52, 255]

> Q：合并（加入残差啊思想）的目的：
> 
> 在每一种维度输出之前还有一个分支就是和下一路进行 concat 拼接 (上一层进行上采样后拼接)。这样加入残差思想，保留各种维度特征 (底层像素 + 高层语义)。三个尺度就可以预测各种不同大小的物体了。

#### 结构模型示意图

![](https://img-blog.csdnimg.cn/616c3e45b5174dcb9a210c348a17fbcf.png)

YOLOv3 总共输出 3 个特征图，第一个特征图下采样 32 倍，第二个特征图下采样 16 倍，第三个下采样 8 倍。输入图像经过 Darknet-53（无全连接层），再经过 YOLOblock 生成的特征图被当作两用，第一用为经过 3×3 卷积层、1×1 卷积之后生成特征图一，第二用为经过 1×1 卷积层加上采样层，与 Darnet-53 网络的中间层输出结果进行拼接，产生特征图二。同样的循环之后产生特征图三。

### 2.4 Feature Extractor—特征提取

### 翻译

我们使用一个新的网络来进行特征提取。

我们的新网络是 YOLOv2 中使用的网络、Darknet-19 和新式的 Darknet 网络之间的一种混合方法。我们的网络使用连续的 3×3 和 1×1 卷积层，但现在也有一些快捷连接，而且明显更大。它有 53 个卷积层，所以我们把它叫做… 等待它… Darknet-53!

![](https://img-blog.csdnimg.cn/baed6e5905954663b1f6f899777e9cdb.png)

这个新网络比 Darknet19 强大得多，但仍然比 ResNet-101 或 ResNet-152 更有效率。下面是一些 ImageNet 的结果:

![](https://img-blog.csdnimg.cn/9ab7a922f98e47799e522c63d9b76155.png)

### 精读

作者是将 darknet-19 里加入了 ResNet 残差连接，改进之后的模型叫 Darknet-53

#### **Darknet-53 主要做了如下改进：**

  （1）**没有采用最大池化层**，转而采用步长为 2 的卷积层进行下采样。

  （2）为了防止过拟合，在每个卷积层之后加入了**一个 BN 层和一个 Leaky ReLU**。

  （3）引入了**残差网络**的思想，目的是为了让网络可以提取到更深层的特征，同时避免出现梯度消失或爆炸。

  （4）**将网络的中间层和后面某一层的上采样进行张量拼接**，达到多尺度特征融合的目的。

Darknet-53 的性能与最先进的分类器相当，但浮点运算更少，速度更快。Darknet-53 还实现了每秒最高的浮点运算。这意味着网络结构更好地利用了 GPU，使其评估更高效，从而更快。

### 2.5 Training—训练

### 翻译

我们仍然在完整的图像上进行训练，没有硬性的负面挖掘或任何这些东西。我们使用多尺度训练，大量的数据增强，批量归一化，所有标准的东西。我们使用 Darknet 神经网络框架进行训练和测试 [14]。

### 精读

（1）训练完整的图像，没有硬负面挖掘。

（2）使用多尺度的训练，大量的数据扩充，批量标准化。

（3）使用 Darknet 神经网络框架来训练和测试。

三、How We Do—我们怎样做
-----------------

### 翻译

YOLOv3 是相当不错的，见表 3。在 COCOs 怪异的平均 AP 指标方面，它与 SSD 的变体相当，但速度快了 3 倍。虽然在这个指标上，YOLOv3 和 RetinaNet 等模型一样。

然而，相比于 IOU=0.5（或图表中的 AP50）的 "老" 检测指标 mAP 时，YOLOv3 显得非常强大。它几乎与 RetinaNet 持平，远高于 SSD 的变体。这表明 YOLOv3 是一个非常强大的检测器，擅长为目标生成合理的框。然而，随着 IOU 阈值的增加，性能明显下降，表明 YOLOv3 努力使框与目标完全对齐。

在过去，YOLO 在处理小物体时很吃力。然而，现在我们看到这一趋势发生了逆转。通过新的多尺度预测，我们看到 YOLOv3 具有相对较高的 APS 性能。然而，它在中等和较大尺寸物体上的性能相对较差。需要进行更多的调查来了解这个问题的真相。

当我们在 AP50 指标上绘制准确度与速度的关系时（见图 5），我们看到 YOLOv3 比其他检测系统有明显的优势。也就是说，它更快、更好。

### 精读

![](https://img-blog.csdnimg.cn/baff8e3b56b64b52b5365680b514be26.png)

> **APs：** 小目标（area(框大小）<32×32）的 AP
> 
> **APm：** 中目标（32×32<area<96×96）的 AP
> 
> **APl：** 大目标（96×96<area）的 AP

#### 结论

*    YOLOv3 擅于预测出「合适」，但无法预测出非常精准的边界框。
*    YOLOv3 小目标预测能力提升，但中大目标的预测反而相对较差。
*    若将速度考量进来，YOLOv3 整体来说表现非常出色。

#### YOLOv3 在小目标 \ 密集目标的改进

 1.**grid cell 个数增加**，YOLOv1（7×7），YOLOv2（13×13），YOLOv3（13×13+26×26+52×52）

 2.**YOLOv2 和 YOLOv3 可以输入任意大小的图片**，输入图片越大，产生的 grid cell 越多，产生的预测框也就越多

 3. **专门小目标预先设置了一些固定长宽比的 anchor**，直接生成小目标的预测框是比较难的，但是在小预测框基础上再生成小目标的预测框是比较容易的

 4. **多尺度预测（借鉴了 FPN）**，既发挥了深层网络的特化语义特征，又整合了浅层网络的细腻度的像素结构信息

 5. 对于小目标而言，边缘轮廓是非常重要的，即浅层网络的边缘信息。**在损失函数中有着惩罚小框项**

 6. 网络结构：**网络加了跨层连接和残差连接（shortcut connection）**，这样可以整合各个层的特征，这样使得网络本身的特征提取能力提升了

**四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法**
------------------------------------------------------

### **翻译**

我们在做 YOLOv3 的时候尝试了很多东西。很多东西都没有成功。以下是我们能记住的东西。

​ **锚框的 X、Y 偏移量预测**。我们尝试使用正常的锚定框预测机制，即用线性激活的方式将 x、y 偏移量预测为框宽或框高的倍数。我们发现这种提法降低了模型的稳定性，而且效果不是很好。

​ **线性 x，y 预测，而不是逻辑预测**。我们尝试用线性激活来直接预测 x，y 偏移量，而不是用逻辑激活。这导致了 mAP 下降了几个点。

​ **Focal loss**。我们尝试使用焦点损失。它使我们的 mAP 下降了大约 2 点。YOLOv3 可能已经对焦点损失试图解决的问题很稳健，因为它有独立的对象性预测和条件类预测。因此，对于大多数例子来说，类别预测没有损失？还是什么？我们并不完全确定。

​ **双重 IOU 阈值和真实分配**。Faster R-CNN 在训练过程中使用两个 IOU 阈值。如果一个预测与真实框重叠 0.7，它就是一个正面的例子，重叠 [0.3 - 0.7]，它就会被忽略，对于所有地面真相对象来说，小于 0.3 就是一个负面的例子。我们尝试过类似的策略，但没有得到好的结果。

我们相当喜欢我们目前的表述，它似乎至少处于一个局部最优状态。这些技术中的一些可能最终会产生好的结果，也许它们只是需要一些调整来稳定训练。

![](https://img-blog.csdnimg.cn/2e37c9eccdc74186a1f34854ab3f0837.png)

### **精读**

这部分不用深入研究，因为这是作者也没有捣鼓清楚的内容~

（1）**预测相对于初始 anchor 宽高倍数的偏移量** ，使用线性激活预测 x, y 偏移为的 anchor box 的宽度或高度的倍数。这种方法降低了模型的稳定性，效果不是很好

（2）**使用线性激活来直接预测 x, y 偏移量**，而不是逻辑逻辑激活。这导致了一些 mAP 的下降。

（3）**Focal loss** （用于图像领域解决数据不平衡造成的模型性能问题，也就是正负样本不均衡，正样本少的问题)。mAPx 下降 2 个点。YOLOv3 可能已经对 focal loss 试图解决的问题很健壮，因为它有独立的目标预测和条件类预测。

（4）**Faster RCNN 在训练中使用两个 IOU 阈值**。如果预测框与真值 IOU 大于 0.7 的，则边界框作为正样本。如果 IOU 在 0.3-0.7 之间它被忽略，小于 0.3 阈值时，它是一个负样本。我们尝试了类似的策略，但没有得到好的结果。

**五、 What This All Means—这一切意味着什么？**
------------------------------------

### 翻译

YOLOv3 是一个好的检测器。它的速度很快，很准确。它在 0.5 和 0.95 IOU 之间的 COCO 平均 AP 指标上不那么好。但它在旧的检测指标 0.5 IOU 上是非常好的。

我们到底为什么要转换指标？最初的 COCO 论文中只有这样一句话：" 关于评估指标的全面讨论。“一旦评估服务器完成，将增加对评估指标的全面讨论”。Russakovsky 等人报告说，人类很难区分 0.3 和 0.5 的 IOU! “训练人类目测一个 IOU 为 0.3 的边界框并将其与一个 IOU 为 0.5 的边界框区分开来是非常困难的。” [18] 如果人类很难区分，那么这又有多大关系呢？但也许一个更好的问题是：“既然我们有了这些探测器，我们要用它们做什么？” 很多做这项研究的人都在谷歌和 Facebook。

我想至少我们知道这项技术是在良好的手中，绝对不会被用来收集你的个人信息并出售给…，等等，你是说这正是它将被用来做什么？哦。

好吧，其他大量资助视觉研究的人是军方，他们从来没有做过任何可怕的事情，比如用新技术杀死很多人，哦，等等… 1，我很希望大多数使用计算机视觉的人只是在用它做快乐的好事，比如计算国家公园里斑马的数量 [13]，或者跟踪他们的猫在家里徘徊 [19]。但是，计算机视觉已经被用于可疑的用途，作为研究人员，我们有责任至少考虑我们的工作可能造成的伤害，并想办法减轻它。我们欠世界这么多。

最后，请不要 @我。(因为我终于退出了 Twitter）。

### 精读

其实这一段也就是作者表达了自己的三观：

作者希望计算机视觉可以用在好的、对的事情上面。Love&Peace~

总结 
---

<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td></td><td>YOLOv1</td><td>YOLOv2</td><td>YOLOv3</td></tr><tr><td>输入图像尺寸</td><td>输入的是 448×448 的三通道图像</td><td>输入的是 416×416 的三通道图像</td><td>输入的是 416×416 的三通道图像</td></tr><tr><td>grid cell</td><td>每一张图像划分为 7×7=49 个 grid cell</td><td>每一张图像划分为 13×13=169 个 grid cell</td><td>yolov3 会产生三个尺度：13×13、26×26、52×52，也对应着 grid cell 个数。</td></tr><tr><td>bbox/anchor</td><td>每个 grid cell 生成 2 个 bbox（没有 anchor），与真实框 IOU 最大的那个框负责拟合真实框</td><td>每个 grid cell 生成 5 个 anchor 框，通过 IOU 计算选一个 anchor 产生预测框去拟合真实框</td><td>每个 grid cell 生成 3 个 anchor 框，通过与 gt 的 IOU 计算选一个 anchor 产生预测框去拟合真实框</td></tr><tr><td>输出张量</td><td>输出 7 * 7 * 30 维的张量 (30 的含义: 两组 bbox 的 xywh 和置信度 + 20 个类别)</td><td>输出 13 * 13 * 125 维张量 (125 的含义：五组 anchor 的 xywh 和置信度 + 20 个类别)</td><td>输出三个不同尺寸的张量，但最后都是 255，比如 S * S * 255，（255 含义: 三组 anchor 里 xywh + 置信度 + 分类数 (COCO 数据集 80 个分类)，所以就是 3 * (80+5)。）</td></tr><tr><td>预测框数量</td><td>7 * 7 *2 = 98 个预测框</td><td>13 * 13 * 5 = 845 个预测框</td><td>(52 * 52 + 26 * 26 +13 * 13) * 3 = 10647 个预测框&nbsp;</td></tr></tbody></table>

本篇结束了，我们 YOLOv4 见~