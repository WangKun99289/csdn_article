> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [blog.csdn.net](https://blog.csdn.net/weixin_43334693/article/details/129011644?spm=1001.2014.3001.5501)

![](https://img-blog.csdnimg.cn/c3aef89434e74d20b979748b2ebb65b7.gif)
---------------------------------------------------------------------

前言
--

从这篇开始，我们将进入 [YOLO](https://so.csdn.net/so/search?q=YOLO&spm=1001.2101.3001.7020) 的学习。YOLO 是目前比较流行的目标检测算法，速度快且结构简单，其他的目标检测算法如 RCNN 系列，以后有时间的话再介绍。

本文主要介绍的是 YOLOV1，这是由以 Joseph Redmon 为首的大佬们于 2015 年提出的一种新的[目标检测算法](https://so.csdn.net/so/search?q=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95&spm=1001.2101.3001.7020)。它与之前的目标检测算法如 R-CNN 等不同之处在于，R-CNN 等目标检测算法是两阶段算法， 步骤为先在图片上生成候选框，然后利用分类器对这些候选框进行逐一的判断；而 YOLOv1 是一阶段算法，是端到端的算法，它把目标检测问题看作回归问题，将图片输入单一的神经网络，然后就输出得到了图片的物体边界框，即 boundingbox 以及分类概率等信息。下面我们就开始学习吧。

下面是一些学习资料：

论文链接：[[1506.02640] You Only Look Once: Unified, Real-Time Object Detection (arxiv.org)](https://arxiv.org/abs/1506.02640 "[1506.02640] You Only Look Once: Unified, Real-Time Object Detection (arxiv.org)")

项目地址 ：[YOLO: Real-Time Object Detection (pjreddie.com)](https://pjreddie.com/darknet/yolo/ "YOLO: Real-Time Object Detection (pjreddie.com)")

Github 源码地址：[mirrors / alexeyab / darknet · GitCode](https://gitcode.net/mirrors/alexeyab/darknet?utm_source=csdn_github_accelerator "mirrors / alexeyab / darknet · GitCode") 

**目录**
------

[Abstract—摘要](#Abstract%E2%80%94%E6%91%98%E8%A6%81%C2%A0) 

[一、Introduction—前言](#t0) 

[二、Uniﬁed Detection—统一检测](#t1)

[2.1 Network Design—网络设计](#t2)

[2.2 Training—训练](#t3) 

[2.3 Inference—推论](#t4)

[2.4 Limitations of YOLO—YOLO 的局限性](#t5) 

[三、Comparison to Other Detection Systems—与其他目标检测算法的比较](#t6)

[四、Experiments—实验](#t7) 

[4.1 Comparison to Other RealTime Systems—与其他实时系统的比较](#t8)

 [4.2 VOC 2007 Error Analysis—VOC 2007 误差分析](#t9)

 [4.3 Combining Fast R-CNN and YOLO—Fast R-CNN 与 YOLO 的结合](#t10)

[4.4 VOC 2012 Results—VOC 2012 结果](#t11) 

[4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测](#t12) 

[五、Real-Time Detection In The Wild—自然环境下的实时检测](#t13) 

[六、Conclusion—结论](#t14)

Abstract—摘要 
------------

### 翻译

我们提出的 [YOLO](https://so.csdn.net/so/search?q=YOLO&spm=1001.2101.3001.7020 "YOLO") 是一种新的目标检测方法。以前的目标检测方法通过重新利用分类器来执行检测。与先前的方案不同，我们将目标检测看作回归问题从空间上**定位边界框（bounding box）**并**预测该框的类别概率**。我们使用**单个神经网络**，在一次评估中直接从完整图像上预测边界框和类别概率。由于整个检测流程仅用一个网络，所以可以直接对检测性能进行**端到端**的优化。

我们的统一架构速度极快。我们的基本 YOLO 模型以 **45 fps**（帧 / 秒）的速度实时处理图像。该网络的一个较小版本——Fast YOLO，以 **155 fps** 这样惊人的速度运行，同时仍然达到其他实时检测器的两倍。与最先进的（state-of-the-art，SOTA）检测系统相比，YOLO 虽然产生了较多的定位误差，但它几乎不会发生把背景预测为目标这样的假阳性（False Positive）的错误。最后，YOLO 能**学习到泛化性很强的目标表征**。当从自然图像学到的模型用于其它领域如艺术画作时，它的表现都优于包括 DPM 和 R-CNN 在内的其它检测方法。

### 精读

#### 之前的方法（RCNN 系列）

（1）通过 region proposal 产生大量的可能包含待检测物体的 **potential bounding box**

（2）再用分类器去判断每个 **bounding box** 里是否包含有物体，以及物体所属类别的 **probability 或者 confidence**

（3）最后回归预测

#### YOLO 的简介：

本文将检测变为一个 regression problem（回归问题），YOLO 从输入的图像，仅仅经过一个神经网络，直接得到一些 bounding box 以及每个 bounding box 所属类别的概率。

因为整个的检测过程仅仅有一个网络，所以它可以直接进行 end-to-end 的优化。

> **end-to-end：** 端到端，指的是输入原始数据，输出的是最后结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征。通过缩减人工预处理和后续处理，尽可能使模型从原始输入到最终输出，给模型更多可以根据数据自动调节的空间，增加模型的整体契合度。在 CV 中具体表现是，神经网络的输入为原始图片，神经网络的输出为（可以直接控制机器的）控制指令。

**一、Introduction—前言** 
----------------------

### 翻译

人们只需瞄一眼图像，立即知道图像中的物体是什么，它们在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使得我们在无意中就能够执行复杂的任务，如驾驶。快速且准确的目标检测算法可以让计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并解锁通用、响应性的机器人系统的潜能。

目前的检测系统通过**重用分类器**来执行检测。为了检测目标，这些系统为该目标提供一个分类器，在测试图像的不同的位置和不同的尺度上对其进行评估。像 deformable parts models（**DPM**，可变形部分模型）这样的系统使用**滑动窗口方法**，其分类器在整个图像上**均匀间隔**的位置上运行 [10]。

最近的方法，如 R-CNN 使用 **region proposal（区域候选）**策略，首先在图像中生成潜在的边界框（bounding box），然后在这些框上运行分类器。在分类之后，执行用于细化边界框的后处理，消除重复的检测，并根据场景中的其它目标为边界框重新打分 [13]。这些复杂的流程是很慢，很难优化的，因为**每个独立的部分都必须单独进行训练**。

**我们将目标检测看作是一个单一的回归问题，直接从图像像素得到边界框坐标和类别概率。**使用我们的系统——You Only Look Once（YOLO），便能得到图像上的物体是什么和物体的具体位置。

YOLO 非常简单（见图 1），它仅用单个卷积网络就能同时预测多个边界框和它们的类别概率。YOLO **在整个图像上训练，并能直接优化检测性能**。与传统的目标检测方法相比，这种统一的模型下面所列的**一些优点**

**第一，YOLO 速度非常快**。由于我们将检测视为**回归问题**，所以我们不需要复杂的流程。测试时，我们在一张新图像上简单的运行我们的神经网络来预测检测结果。在 Titan X GPU 上不做批处理的情况下，YOLO 的基础版本以每秒 45 帧的速度运行，而快速版本运行速度超过 150fps。这意味着我们可以在不到 25 毫秒的延迟内实时处理流媒体视频。此外，YOLO 实现了其它实时系统两倍以上的平均精度。关于我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：[YOLO: Real-Time Object Detection](http://pjreddie.com/yolo/ "YOLO: Real-Time Object Detection")。

**第二，YOLO 是在****整个图像****上进行推断的**。与基于滑动窗口和候选框的技术不同，YOLO 在训练期间和测试时都会顾及到整个图像，所以它隐式地包含了关于类的上下文信息以及它们的外观。Fast R-CNN 是一种很好的检测方法 [14]，但由于它看不到更大的上下文，会将背景块误检为目标。与 Fast R-CNN 相比，YOLO 的背景误检数量少了一半。

**第三，YOLO 能学习到****目标的泛化表征**（generalizable representations of objects）。把在自然图像上进行训练的模型，用在艺术图像进行测试时，YOLO 大幅优于 DPM 和 R-CNN 等顶级的检测方法。由于 YOLO 具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。

**YOLO 在精度上仍然落后于目前最先进的检测系统**。虽然它可以快速识别图像中的目标，但它在定位某些物体尤其是小的物体上精度不高。

我们在实验中会进一步探讨精度／时间的权衡。我们所有的训练和测试代码都是开源的，而且各种预训练模型也都可以下载。

### 精读 

#### 之前的研究：

**DPM：** 系统为检测对象使用分类器，并在测试图像的不同位置和尺度对其进行评估

**R-CNN：**SS 方法提取候选框＋CNN＋分类 + 回归。

#### YOLO 处理步骤：

![](https://img-blog.csdnimg.cn/2aa04ca2a8634a2ca033a871cc346699.png)

(1) 将输入图像的大小调整为 448×448，分割得到 7*7 网格；

(2) 通过 CNN 提取特征和预测；

(3) 利用非极大值抑制（NMS）进行筛选

#### YOLO 的定义：

YOLO 将目标检测重新定义为**单个回归问题**，**从图像像素直接到边界框坐标和类概率**。YOLO 可以在一个图像来预测：哪些对象是存在的？它们在哪里？

如 Figure 1：将图像输入单独的一个 CNN 网络，就会预测出 bounding box，以及这些 bounding box 所属类别的概率。

YOLO 用**一整幅图像**来训练，同时可以直接优化性能检测。

性能检测对比：

![](https://img-blog.csdnimg.cn/f29e9b3e00fe4222b5eb494673611e6d.png)

#### YOLO 的优点：

（1）**YOLO 的速度非常快。**能够达到实时的要求。在 Titan X 的 GPU 上 能够达到 45 帧每秒。

（2）**YOLO 在做预测时使用的是全局图像。**与 FastR-CNN 相比，YOLO 产生的背景错误数量不到一半。

（3）**YOLO 学到物体更泛化的特征表示。**因此当应用于新域或意外输入时，不太可能崩溃。

**二、Uniﬁed Detection—统一检测**
---------------------------

### 网格单元 

### 翻译

**我们将目标检测的独立部分 (the separate components) 整合到单个神经网络中**。我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测一张图像中的所有类别的所有边界框。这意味着我们的网络对整张图像和图像中的所有目标进行全局推理 (reason globally)。YOLO 设计可实现端到端训练和实时的速度，同时保持较高的平均精度。

**我们的系统将输入图像分成 S×S 的网格**。如果目标的中心落入某个网格单元 (grid cell) 中，那么该网格单元就负责检测该目标。

**每个网格单元都会预测 B 个 边界框和这些框的置信度分数（confidence scores）**。这些置信度分数反映了该模型对那个框内是否包含目标的置信度，以及它对自己的预测的准确度的估量。在形式上，我们将**置信度**定义为 confidence=Pr(Object)∗IOUpred truth ​。如果该单元格中不存在目标（即 Pr(Object)=0），则置信度分数应为 0 。否则（即 Pr(Object)=1），我们希望置信度分数等于预测框（predict box）与真实标签框（ground truth）之间联合部分的交集（IOU）。

每个网格单元还预测了 C 类的条件概率，Pr(Classi|Object)。这些概率是以包含目标的网格单元为条件的。我们只预测每个网格单元的一组类别概率，而不考虑框 B 的数量。

在测试时，我们将条件类概率和单个框的置信度预测相乘：![](https://latex.csdn.net/eq?Pr%28Class_%7Bi%7D%7CObject%29*Pr%28Object%29*IOU%5E%7Btruth%7D_%7Bpred%7D%3DPr%28Class_%7Bi%7D%29*IOU%5E%7Btruth%7D_%7Bpred%7D)

这给我们提供了每个框的特定类别的置信度分数。这些分数既是对该类出现在框里的概率的编码，也是对预测的框与目标的匹配程度的编码。

### 精读

#### 思想

YOLO 将目标检测问题作为**回归问题**。会将输入图像分成 S×S 的网格，如果一个物体的中心点落入到一个 cell 中，那么该 cell 就要负责预测该物体，一个格子只能预测一个物体，会生成两个预测框。

#### 对于每个 grid cell：

（1）预测 B 个边界框，每个框都有一个置信度分数（confidence score）这些框大小尺寸等等都随便，只有一个要求，就是**生成框的中心点必须在 grid cell 里**。

（2）每个边界框包含 5 个元素：**(x,y,w,h)**

![](https://img-blog.csdnimg.cn/2212accd4ab2420e9c1ba9eff85355cc.png)

> **● x，y：** 是指 bounding box 的预测框的中心坐标相较于该 bounding box 归属的 grid cell 左上角的偏移量，在 0-1 之间。
> 
> ![](https://img-blog.csdnimg.cn/6dd06d7f262843ecb30140c1e5ae5cfd.png)
> 
> 在上图中，**绿色虚线框**代表 grid cell，**绿点**表示该 grid cell 的左上角坐标，为（0，0）；**红色和蓝色框**代表该 grid cell 包含的两个 bounding box，**红点和蓝点**表示这两个 bounding box 的中心坐标。有一点很重要，bounding box 的中心坐标一定在该 grid cell 内部，因此，红点和蓝点的坐标可以归一化在 0-1 之间。在上图中，红点的坐标为（0.5，0.5），即 x=y=0.5，蓝点的坐标为（0.9，0.9），即 x=y=0.9。

> **● w，h：** 是指该 bounding box 的宽和高，但也归一化到了 0-1 之间，表示相较于原始图像的宽和高（即 448 个像素）。比如该 bounding box 预测的框宽是 44.8 个像素，高也是 44.8 个像素，则 w=0.1，h=0.1。
> 
> ![](https://img-blog.csdnimg.cn/111dbe8a5dc449bd8e0d74cc34a21c4b.png)
> 
> **红框**的 x=0.8，y=0.5，w=0.1，h=0.2。

（3）不管框 B 的数量是多少，**只负责预测一个目标**。

（4）预测 C 个条件概率类别（物体属于每一种类别的可能性）

![](https://img-blog.csdnimg.cn/ad8627eedb30408084fab8c79c801993.png)

**综上，S×S 个网格，每个网格要预测 B 个 bounding box （中间上图），还要预测 C 个类（中间下图）。**将两图合并，网络输出就是一个 S × S × (5×B+C)。（S x S 个网格，每个网格都有 B 个预测框，每个框又有 5 个参数，再加上每个网格都有 C 个预测类）

> Q1：为什么每个网格有固定的 B 个 bounding box？（即 B=2）
> 
> 在训练的时候会在线地计算每个 predictor 预测的 bounding box 和 ground truth 的 IOU，计算出来的 IOU 大的那个 predictor，就会负责预测这个物体，另外一个则不预测。这么做有什么好处？我的理解是，这样做的话，实际上有两个 predictor 来一起进行预测，然后网络会在线选择预测得好的那个 predictor（也就是 IOU 大）来进行预测。

> Q2：每个网格预测的两个 bounding box 是怎么得到的？
> 
> YOLO 中两个 bounding box 是人为选定的 (2 个不同 长宽比）的 box，在训练开始时作为超参数输入 bounding box 的信息，随着训练次数增加，loss 降低，bounding box 越来越准确。Faster [RCNN](https://so.csdn.net/so/search?q=RCNN&spm=1001.2101.3001.7020) 也是人为选定的（9 个 不同长宽比和 scale），YOLOv2 是统计分析 ground true box 的特点得到的（5 个）。

#### 预测特征组成

最终的预测特征由边框的位置、边框的置信度得分以及类别概率组成，这三者的含义如下：

*   **边框位置：** 对每一个边框需要预测其中心坐标及宽、高这 4 个量， 两个边框共计 8 个预测值边界框宽度 w 和高度 h 用图像宽度和高度归一化。因此 x,y,w,h 都在 0 和 1 之间。
*   **置信度得分 (box confidence score) c ：** 框包含一个目标的可能性以及边界框的准确程度。类似于 Faster RCNN 中是前景还是背景。由于有两个边框，因此会存在两个置信度预测值。
*   **类别概率：** 由于 PASCAL VOC 数据集一共有 20 个物体类别，因此这里预测的是边框属于哪一个类别。

#### 注意

*   一个 cell 预测的两个边界框共用一个类别预测， 在**训练**时会选取与标签 IoU 更大的一个边框负责回归该真实物体框，在**测试**时会选取置信度更高的一个边框，另一个会被舍弃，因此 7×7=49 个 gird cell 最多只能预测 49 个物体。
*   因为每一个 grid cell 只能有一个分类，也就是他只能预测一个物体，这也是导致 YOLO 对小目标物体性能比较差的原因。**如果所给图片极其密集，导致 grid cell 里可能有多个物体，但是 YOLO 模型只能预测出来一个，那这样就会忽略在本 grid cell 内的其他物体。**
    
    * * *
    
    ### 2.1 Network Design—网络设计
    

###  **翻译**

我们将此模型作为卷积神经网络来实现，并在 Pascal VOC 检测数据集 [9] 上进行评估。网络的初始卷积层从图像中提取特征，而全连接层负责预测输出概率和坐标。

我们的网络架构受图像分类模型 **GoogLeNet** 的启发 [34]。我们的网络有 24 个卷积层，后面是 2 个全连接层。我们只使用 1×1 降维层，后面是 3×3 卷积层，这与 Lin 等人[22] 类似，而不是 GoogLeNet 使用的 Inception 模块。

我们还训练了快速版本的 YOLO，旨在推动快速目标检测的界限。快速 YOLO 使用具有较少卷积层（9 层而不是 24 层）的神经网络，在这些层中使用较少的卷积核。除了网络规模之外，基本版 YOLO 和快速 YOLO 的所有训练和测试参数都是相同的。

我们网络的最终输出是 7×7×30 的预测张量。

### 精读

#### 网络结构

YOLO 网络结构借鉴了 GoogLeNet （[经典神经网络论文超详细解读（三）——GoogLeNet InceptionV1 学习笔记（翻译＋精读 + 代码复现）](https://blog.csdn.net/weixin_43334693/article/details/128267380 "经典神经网络论文超详细解读（三）——GoogLeNet InceptionV1学习笔记（翻译＋精读+代码复现）")）。输入图像的尺寸为 448×448，经过 24 个卷积层，2 个全连接的层（FC），最后在 reshape 操作，输出的特征图大小为 7×7×30。

![](https://img-blog.csdnimg.cn/401d4ebcdda84139a372f00079967031.png)

> Q：7×7×30 怎么来的？
> 
> 张量剖面图
> 
> ![](https://img-blog.csdnimg.cn/b291114a6c0944d1ae20badb6d35209c.png)
> 
>  （图片来源：[YOLO v1 详细解读_yolov1 详解_迪菲赫尔曼的博客 - CSDN 博客](https://yolov5.blog.csdn.net/article/details/123523225 "YOLO v1详细解读_yolov1详解_迪菲赫尔曼的博客-CSDN博客")）
> 
> *    **7×7：** 一共划分成 7×7 的网格。
> *    **30：** 30 包含了两个预测框的参数和 Pascal VOC 的类别参数：每个预测框有 5 个参数：x,y,w,h,confidence。另外，Pascal VOC 里面还有 20 个类别；所以最后的 30 实际上是由 5x2+20 组成的，也就是说这一个 30 维的向量就是一个 gird cell 的信息。
> *    **7×7×30：** 总共是 7 × 7 个 gird cell 一共就是 7 × 7 ×（2 × 5+ 20）= 7 × 7 × 30 tensor = 1470 outputs，正好对应论文。

#### 网络详解

（1）YOLO 主要是建立一个 CNN 网络生成预测 **7×7×1024** 的张量 。

（2）然后使用两个全连接层执行线性回归，以进行 **7×7×2** 边界框预测。将具有高置信度得分（大于 0.25）的结果作为最终预测。

（3）在 3×3 的卷积后通常会接一个通道数更低 **1×1** 的卷积，这种方式既降低了计算量，同时也提升了模型的非线性能力。

（4）除了最后一层使用了线性激活函数外，其余层的激活函数为 **Leaky ReLU** 。

（5）在训练中使用了 **Dropout** 与数据增强的方法来防止过拟合。

（6）对于最后一个卷积层，它输出一个形状为 **(7, 7, 1024)** 的张量。 然后张量展开。使用 2 个全连接层作为一种线性回归的形式，它输出 1470 个参数，然后 reshape 为 **(7, 7, 30)** 。

### 2.2 Training—训练 

### 翻译

我们在 ImageNet 的 1000 类竞赛数据集 [30] 上**预训练我们的卷积层**。对于预训练，我们使用图 3 中的前 20 个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在 ImageNet 2012 验证集上获得了单一裁剪图像 88% 的 top-5 准确率，与 Caffe 模型池中的 GoogLeNet 模型相当。我们使用 **Darknet** 框架进行所有的训练和推断 [26]。

然后我们转换模型来执行检测训练。Ren 等人表明，**预训练网络中增加卷积层和连接层可以提高性能** [29]。按照他们的方法，我们**添加了四个卷积层和两个全连接层，这些层的权重都用随机值初始化**。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从 224×224 改为 448×448。

**模型的最后一层预测类概率和边界框坐标**。我们通过图像宽度和高度来规范边界框的宽度和高度，使它们落在 0 和 1 之间。我们将边界框 x 和 y 坐标参数化为特定网格单元位置的**偏移量**，所以它们的值被限定在在 0 和 1 之间。

**模型的最后一层**使用线性激活函数，而**所有其它的层**使用下面的 Leaky-ReLU：

![](https://latex.csdn.net/eq?%5Cphi%20%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20if%20%5C%3A%20x%3E0%20%26%20%26%20%5C%5C0.1x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20otherwise%26%20%26%20%5Cend%7Bmatrix%7D%5Cright.)![](https://latex.csdn.net/eq?%5Cphi%20%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20if%20%5C%3A%20x%3E0%20%26%20%26%20%5C%5C0.1x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20otherwise%26%20%26%20%5Cend%7Bmatrix%7D%5Cright.)

我们对模型输出的**平方和误差** (sum-squared error) 进行优化。我们选择使用平方和误差，是因为它易于优化，但是它并不完全符合最大化平均精度（average precision）的目标。它给分类误差与定位误差的权重是一样的，这点可能并不理想。**另外**，每个图像都有很多网格单元并没有包含任何目标，这将这些单元格的 “置信度” 分数推向零，通常压制了包含目标的单元格的梯度。这可能导致模型不稳定，从而导致训练在早期就发散(diverge)。

为了弥补平方和误差的缺陷，我们增加了边界框坐标预测的损失，并减少了不包含目标的框的置信度预测的损失。 我们使用两个参数λ c o o r d λ_{coord}λcoord​和λ n o o b j λ_{noobj}λnoobj​来实现这一点。 我们设定λ c o o r d = 5 λ_{coord}= 5λcoord​=5 和 λ n o o b j = 0.5 λ_{noobj}=0.5λnoobj​=0.5。

平方和误差对大框和小框的误差权衡是一样的，而我们的错误指标 (error metric) 应该要体现出，大框的小偏差的重要性不如小框的小偏差的重要性。为了部分解决这个问题，我们直接预测边界框宽度和高度的**平方根**，而不是宽度和高度。

**YOLO 为每个网格单元预测多个边界框**。在训练时，每个目标我们只需要一个边界框预测器来负责。若某预测器的预测值与目标的实际值的 IOU 值最高，则这个预测器被指定为 “负责” 预测该目标。这导致边界框预测器的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率(recall)。

**在训练期间，我们优化以下多部分损失函数：**

**![](https://img-blog.csdnimg.cn/21c6e1125df74bbbbd9f1eb19d23d5cf.png)**

 注意，如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚 (penalizes) 分类错误。如果预测器 “负责” 实际边界框（即该网格单元中具有最高 IOU 的预测器），则它也仅惩罚边界框坐标错误。

我们用 Pascal VOC 2007 和 2012 的训练集和验证数据集进行了大约 135 个 epoch 的网络训练。因为我们仅在 Pascal VOC 2012 上进行测试，所以我们的训练集里包含了 Pascal VOC 2007 的测试数据。在整个训练过程中，我们使用：`batch size=64，momentum=0.9，decay=0.0005`。

我们的学习率（`learning rate`）计划如下：在第一个 epoch 中，我们将学习率从 1 0 − 3 10^{-3}10−3 慢慢地提高到 1 0 − 2 10^{-2}10−2。如果从大的学习率开始训练，我们的模型通常会由于不稳定的梯度而发散 (diverge)。我们继续以 1 0 − 2 10^{-2}10−2 进行 75 个周期的训练，然后以 1 0 − 3 10^{-3}10−3 进行 30 个周期的训练，最后以 1 0 − 4 10^{-4}10−4 进行 30 个周期的训练。

**为避免过拟合，我们使用了 Dropout 和大量的数据增强**。 在第一个连接层之后的 dropout 层的丢弃率设置为 0.5，以防止层之间的相互适应 [18]。 对于数据增强 (data augmentation)，我们引入高达 20％的原始图像大小的随机缩放和平移 (random scaling and translations )。我们还在 HSV 色彩空间中以高达 1.5 的因子随机调整图像的曝光度和饱和度。

### 精读

#### 预训练分类网络

在 ImageNet 1000 数据集上预训练一个分类网络，**这个网络使用 Figure3 中的前 20 个卷积层，然后是一个平均池化层和一个全连接层。**（此时网络输入是 224×224）。

> Q：主干结构的输入要求必须是 448x448 的固定尺寸，为什么在预训练阶段可以输入 224x224 的图像呢？
> 
> 主要原因是加入了**平均池化层**，这样不论输入尺寸是多少，在和最后的全连接层连接时都可以保证相同的神经元数目。

![](https://img-blog.csdnimg.cn/42dadd091790446db552d368306e5f7d.png)

####   训练检测网络

经过上一步的预训练，就已经把主干网络的前 20 个卷积层给训练好了，前 20 层的参数已经学到了图片的特征。接下来的步骤本质就是迁移学习，**在训练好的前 20 层卷积层后加上 4 层卷积层和 2 层全连接层，然后在目标检测的任务上进行迁移学习。**

在整个网络（24+2）的训练过程中，除最后一层采用 ReLU 函数外，其他层均采用 leaky ReLU 激活函数。leaky ReLU 相对于 ReLU 函数可以解决在输入为负值时的零梯度问题。YOLOv1 中采用的 leaky ReLU 函数的表达式为：

![](https://latex.csdn.net/eq?%5Cphi%20%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20if%20%5C%3A%20x%3E0%20%26%20%26%20%5C%5C0.1x%2C%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20otherwise%26%20%26%20%5Cend%7Bmatrix%7D%5Cright.)

#### NMS 非极大值抑制

**概念：**NMS 算法主要解决的是一个目标被多次检测的问题，意义主要在于在一个区域里交叠的很多框选一个最优的。

**YOLO 中具体操作**

![](https://img-blog.csdnimg.cn/0524ba33fc0c44c9a2e5d75d200752a4.gif)

 （1）对于上述的 98 列数据，先看某一个类别，也就是只看 98 列的这一行所有数据，**先拿出最大值概率的那个框，剩下的每一个都与它做比较**，如果两者的 IoU 大于某个阈值，则认为这俩框重复识别了同一个物体，就将其中低概率的重置成 0。

（2）最大的那个框和其他的框比完之后，**再从剩下的框找最大的，继续和其他的比**，依次类推对所有类别进行操作。 注意，这里不能直接选择最大的，因为有可能图中有多个该类别的物体，所以 IoU 如果小于某个阈值，则会被保留。

（3）最后得到一个稀疏矩阵，因为里面有很多地方都被重置成 0，拿出来不是 0 的地方拿出来概率和类别，就得到最后的目标检测结果了。

**注意：** NMS 只发生在预测阶段，训练阶段是不能用 NMS 的，因为在训练阶段不管这个框是否用于预测物体的，他都和损失函数相关，不能随便重置成 0。

#### 损失函数

**损失函数包括：**

localization loss -> 坐标损失

confidence loss -> 置信度损失

classification loss -> 分类损失

![](https://img-blog.csdnimg.cn/d9f9674464074751adf757514eaad4a0.png)

#### 损失函数详解：

**（1）坐标损失**

**![](https://img-blog.csdnimg.cn/617c0216c97545499cf9a534b61e3da3.png)**

*   第一行： 负责检测物体的框中心点（x, y）定位误差。
*   第二行： 负责检测物体的框的高宽（w,h) 定位误差，这个根号的作用就是为了修正对大小框一视同仁的缺点，削弱大框的误差。

> Q：为啥加根号？
> 
> ![](https://img-blog.csdnimg.cn/9dbddc8aaf5f400f9940850f28278dcd.png)
> 
> 在上图中，大框和小框的 bounding box 和 ground truth 都是差了一点，但对于实际预测来讲，大框（大目标）差的这一点也许没啥事儿，而小框（小目标）差的这一点可能就会导致 bounding box 的方框和目标差了很远。而如果还是使用第一项那样直接算平方和误差，就相当于把大框和小框一视同仁了，这样显然不合理。而如果使用开根号处理，就会一定程度上改善这一问题 。
> 
> ![](https://img-blog.csdnimg.cn/dee46776e8704ef9856c704d6200609b.png)
> 
> 这样一来，同样是差一点，小框产生的误差会更大，即对小框惩罚的更严重。

**（2）置信度损失**

**![](https://img-blog.csdnimg.cn/9bff9f15ea81493a9d76d18155d9c480.png)** 

*   第一行： 负责检测物体的那个框的置信度误差。
*   第二行： 不负责检测物体的那个框的置信度误差。

**（3）分类损失**

**![](https://img-blog.csdnimg.cn/0a62e05704d74b7dbd97425f7763fce1.png)** 负责检测物体的 grid cell 分类的误差。

#### 特殊符号的含义：

![](https://img-blog.csdnimg.cn/ad92ee666f7241d394525add449d467f.png) 

### 2.3 Inference—推论

### 翻译

就像在训练中一样，预测测试图像的检测只需要一次网络评估。在 Pascal VOC 上，每张图像上网络预测 98 个边界框和每个框的类别概率。YOLO 在测试时非常快，因为它只需要一次网络评估 (network evaluation)，这与基于分类器的方法不同。

**网格设计强化了边界框预测中的空间多样性**。通常一个目标落在哪一个网格单元中是很明显的，而网络只能为每个目标预测一个边界框。然而，一些大的目标或接近多个网格单元的边界的目标能被多个网格单元定位。**非极大值抑制 (Non-maximal suppression,NMS) 可以用来修正这些多重检测**。非最大抑制对于 YOLO 的性能的影响不像对于 R-CNN 或 DPM 那样重要，但也能增加 2−3% 的 mAP。

### 精读

（1）预测测试图像的检测只需要一个网络评估。

（2）测试时间快

（3）当图像中的物体较大，或者处于 grid cells 边界的物体，可能在多个 cells 中被定位出来。

（4）利用 NMS 去除重复检测的物体，使 mAP 提高，但和 RCNN 等相比不算大。

### 2.4 Limitations of YOLO—YOLO 的局限性 

### 翻译

由于每个格网单元只能预测两个框，并且只能有一个类，因此 YOLO 对边界框预测**施加了很强的空间约束**。这个空间约束限制了我们的模型可以预测的邻近目标的数量。我们的模型难以预测群组中出现的小物体（比如鸟群）。

由于我们的模型学习是从数据中预测边界框，因此它**很难泛化**到新的、不常见的长宽比或配置的目标。我们的模型也使用**相对较粗糙的特征**来预测边界框，因为输入图像在我们的架构中**历经了多个下采样层** (downsampling layers)。

最后，我们的训练基于一个逼近检测性能的损失函数，这个损失函数**无差别地**处理小边界框与大边界框的误差。大边界框的小误差通常是无关要紧的，但小边界框的小误差对 IOU 的影响要大得多。我们的**主要错误来自于不正确的定位**。

### 精读

（1）对于图片中一些群体性小目标检测效果比较差。因为 yolov1 网络到后面感受野较大，小目标的特征无法再后面 7×7 的 grid 中体现，**针对这一点，yolov2 已作了一定的修改，加入前层（感受野较小）的特征进行融合。**

（2）原始图片只划分为 7x7 的网格，当两个物体靠的很近时（挨在一起且中点都落在同一个格子上的情况），效果比较差。因为 yolov1 的模型决定了一个 grid 只能预测出一个物体，所以就会丢失目标，**针对这一点，yolov2 引入了 anchor 的概念，一个 grid 有多少个 anchor 理论上就可以预测多少个目标。**

（3）每个网格只对应两个 bounding box，当物体的长宽比不常见 (也就是训练数据集覆盖不到时)，效果较差。

（4）最终每个网格只对应一个类别，容易出现漏检 (物体没有被识别到)。

三、Comparison to Other Detection Systems—与其他目标检测算法的比较
----------------------------------------------------

### 翻译

目标检测是计算机视觉中的核心问题。检测流程通常是**首先**从输入图像上提取一组鲁棒特征（Haar [25]，SIFT [23]，HOG [4]，卷积特征 [6]）。**然后**，分类器 [36,21,13,10] 或定位器 [1,32] 被用来识别特征空间中的目标。这些分类器或定位器或在整个图像上或在图像中的一些子区域上以滑动窗口的方式运行[35,15,39]。我们将 YOLO 检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异性。

**Deformable parts models**。可变形部分模型（DPM）使用**滑动窗口**方法进行目标检测 [10]。DPM 使用**不相交的流程**来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些不同的部分。**网络同时进行特征提取，边界框预测，非极大值抑制和上下文推理**。网络的特征 feature 是在**在线 (in-line)** 训练出来的而不是静态，因此可以根据特定的检测任务进行优化。我们的统一架构比 DPM 更快，更准确。

**R-CNN**。R-CNN 及其变体 (variants) 使用**区域候选**而不是滑动窗口来查找图像中的目标。选择性搜索 [35] 生成潜在的边界框(Selective Search generates potential bounding boxes)，卷积网络提取特征，SVM 对框进行评分，线性模型调整边界框，非最大抑制消除重复检测(eliminates duplicate detections)。 这个复杂流水线的每个阶段都必须独立地进行精确调整(precisely tuned independently)，所得到的系统非常缓慢，在测试时间每个图像需要超过 40 秒[14]。

YOLO 与 R-CNN 有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。然而，我们的系统对网格单元的候选框**施加空间限制**，这有助于缓解对同一目标的多次检测的问题。 我们的系统还**生成了更少的边界框**，每张图像只有 98 个，而选择性搜索则有约 2000 个。最后，我们的系统将这些单独的组件 (individual components) 组合成一个单一的、共同优化的模型。

**其它快速检测器**。 Fast R-CNN 和 Faster R-CNN 通过共享计算和使用神经网络替代选择性搜索 [14]，[28] 来提出候选区域来加速 R-CNN 框架。虽然它们提供了比 R-CNN 更快的速度和更高的准确度，但仍然不能达到实时性能。

许多研究工作集中在加快 DPM 流程上 [31] [38] [5]。它们加速 HOG 计算，使用级联(cascades)，并将计算推动到（多个）GPU 上。但是，实际上只有 30Hz 的 DPM [31] 可以实时运行。

YOLO 并没有试图优化大型检测流程的单个组件，相反，而是完全抛弃 (throws out…entirely) 了大型检测流程，并通过设计来提高速度。

像人脸或行人等单个类别的检测器可以高度优化，因为他们只需处理较少的多样性 [37]。YOLO 是一种通用的检测器，它可以同时(simultaneously) 检测多个目标。

**Deep MultiBox**。与 R-CNN 不同，Szegedy 等人 训练一个卷积神经网络来预测感兴趣的区域 (regions of interest,ROI)[8]，而不是使用选择性搜索。 MultiBox 还可以通过用单个类别预测替换置信度预测来执行单个目标检测。 但是，MultiBox 无法执行一般的目标检测，并且**仍然只是较大检测流水线中的一部分**，需要进一步的图像补丁分类。 YOLO 和 MultiBox 都使用卷积网络来预测图像中的边界框，但 **YOLO 是一个完整的检测系统**。

**OverFeat**。Sermanet 等人训练了一个卷积神经网络来执行定位，并使该定位器进行检测 [32]。OverFeat 高效地执行滑动窗口检测，但它仍然是一个**不相交的系统** (disjoint system)。OverFeat 优化了定位功能，而不是检测性能。像 DPM 一样，定位器在进行预测时只能看到局部信息。OverFeat 无法推断全局上下文，因此需要大量的后处理来产生连贯的检测。

**MultiGrasp**。我们的系统在设计上类似于 Redmon 等 [27] 的抓取检测。 **我们的网格边界框预测方法基于 MultiGrasp 系统进行回归分析**。 然而，抓取检测比物体检测要简单得多。 MultiGrasp 只需要为包含一个目标的图像预测一个可抓取区域。 它不必估计目标的大小，位置或边界或预测它的类别，只需找到适合抓取的区域就可以了。 而 **YOLO 则是预测图像中多个类的多个目标的边界框和类概率**。

### 精读

#### DPM

用传统的 HOG 特征方法，也用的是传统的支持向量机 SVM 分类器，然后人工造一个模板，再用滑动窗口方法不断的暴力搜索整个待识别图，去套那个模板。这个方法比较大的问题就是在于设计模板，计算量巨大，而且是个静态的，没办法匹配很多变化的东西，鲁棒性差。

#### **R-CNN**

*   第一阶段：每个图片使用选择性搜索 SS 方法提取 2000 个候选框。
*   第二阶段：将每个候选框送入 CNN 网络进行分类 (使用的 SVM)。

YOLO 对比他们俩都很强，YOLO 和 R-CNN 也有相似的地方，比如也是提取候选框，YOLO 的候选框就是上面说过的那 98 个 bounding boxes，也是用到了 NMS 非极大值抑制，也用到了 CNN 提取特征。

#### Other Fast Detectors

Fast 和 Faster R-CNN ：这俩模型都是基于 R-CNN 的改版，速度和精度都提升了很多，但是也没办法做到实时监测，也就是说 FPS 到不了 30，作者在这里并没有谈准确度的问题，实际上 YOLO 的准确度在这里是不占优势的，甚至于比他们低。

#### Deep MultiBox

训练卷积神经网络来预测感兴趣区域，而不是使用选择性搜索。多盒也可以用单个类预测替换置信预测来执行单个目标检测。YOLO 和 MultiBox 都使用卷积网络来预测图像中的边界框，但 YOLO 是一个完整的检测系统。

#### OverFeat

OverFeat 有效地执行滑动窗口检测，优化了定位，而不是检测性能。与 DPM 一样，定位器在进行预测时只看到本地信息。OverFeat 不能推理全局环境。

#### MultiGrasp

YOLO 在设计上与 Redmon 等人的抓取检测工作相似。边界盒预测的网格方法是基于多重抓取系统的回归到抓取。

总之，作者就是给前人的工作都数落一遍，凸显自己模型的厉害（学到了！）

**四、Experiments—实验** 
---------------------

### 4.1 Comparison to Other RealTime Systems—与其他实时系统的比较

### 翻译

目标检测方面的许多研究工作都集中在使标准的检测流程更快 [5]，[38]，[31]，[14]，[17]，[28]。然而，只有 Sadeghi 等人实际上产生了一个实时运行的检测系统（每秒 30 帧或更好）[31]。我们将 YOLO 与 DPM 的 GPU 实现进行了比较，其在 30Hz 或 100Hz 下运行。虽然其它的算法没有达到实时性的标准，我们也比较了它们的 **mAP 和速度**的关系，从而探讨目标检测系统中**精度和性能之间的权衡**。

**Fast YOLO** 是 PASCAL 上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有 52.7% 的 mAP，实时检测的精度是以前的方法的两倍以上。**普通版 YOLO** 将 mAP 推到 63.4% 的同时保持了实时性能。

我们还使用 VGG-16 训练 YOLO。 这个模型比**普通版 YOLO** 更精确，但也更慢。 它的作用是与依赖于 VGG-16 的其他检测系统进行比较，但由于它比实时更慢，所以本文的其他部分将重点放在我们更快的模型上。

最快的 DPM 可以在不牺牲太多 mAP 的情况下有效加速 DPM，但仍然会将实时性能降低 2 倍 [38]。与神经网络方法相比，DPM 的检测精度相对较低，这也是限制它的原因。

减去 R 的 R-CNN 用静态侯选边界框取代选择性搜索 [20]。虽然速度比 R-CNN 更快，但它仍然无法实时，并且由于该方法无法找到好的边界框，准确性受到了严重影响。

Fast R-CNN 加快了 R-CNN 的分类阶段，但它仍然依赖于选择性搜索，每个图像需要大约 2 秒才能生成边界候选框。因此，它虽然具有较高的 mAP，但的速度是 0.5 fps，仍然远未达到实时。

最近的 Faster R-CNN 用神经网络替代了选择性搜索来候选边界框，类似于 Szegedy 等人 [8] 的方法。在我们的测试中，他们最准确的模型达到了 7fps，而较小的、不太准确的模型以 18 fps 运行。 Faster R-CNN 的 VGG-16 版本比 YOLO 高出 10mAP，但比 YOLO 慢了 6 倍。 Zeiler-Fergus 版本的 Faster R-CNN 只比 YOLO 慢 2.5 倍，但也不如 YOLO 准确。

### 精读

Table 1 在 Pascal VOC 2007 上与其他检测方法的对比![](https://img-blog.csdnimg.cn/011042ed4669437d99594f11cbaef569.png)

**结论：**实时目标检测（FPS>30），YOLO 最准，Fast YOLO 最快。 

### 4.2 VOC 2007 Error Analysis—VOC 2007 误差分析

### 翻译

为了进一步研究 YOLO 和最先进的检测器之间的差异，我们详细分析了 VOC 2007 的分类 (breakdown) 结果。我们将 YOLO 与 Fast R-CNN 进行比较，因为 Fast R-CNN 是 PASCAL 上性能最高的检测器之一并且它的检测代码是可公开得到的。

我们使用 Hoiem 等人的方法和工具 [19]，对于测试的每个类别，我们查看该类别的前 N 个预测。每个预测都或是正确的，或是根据错误的类型进行分类：

*   Correct: correct class and IOU>0.5
*   Localization: correct class, 0.1<IOU<0.5
*   Similar: class is similar, IOU>0.1
*   Other: class is wrong, IOU>0.1
*   Background: IOU<0.1 for any object（所有目标的 IOU 都 < 0.1）

YOLO 难以正确地定位目标，因此定位错误比 YOLO 的所有其他错误总和都要多。Fast R-CNN 定位错误更少，但把背景误认成目标的错误比较多。它的最高检测结果中有 13.6％是不包含任何目标的误报 (false positive，背景)。 Fast R-CNN 把背景误认成目标的概率比 YOLO 高出 3 倍。 

### 精读

本文使用 HoeMm 等人的方法和工具。对于测试时间的每个类别，查看该类别的 N 个预测。每个预测要么是正确的，要么是基于错误类型进行分类的：

![](https://img-blog.csdnimg.cn/8ab0951cddd240a7b52e92cc35a73bb1.png)

**参数含义：**

*   **Correct：**正确分类，且预测框与 ground truth 的 IOU 大于 0.5，既预测对了类别，预测框的位置和大小也很合适。 
*   **Localization：**正确分类，但预测框与 ground truth 的 IOU 大于 0.1 小于 0.5，即虽然预测对了类别，但预测框的位置不是那么的严丝合缝，不过也可以接受。
*   **Similar：** 预测了相近的类别，且预测框与 ground truth 的 IOU 大于 0.1。即预测的类别虽不正确但相近，预测框的位置还可以接受。
*   **Other：**预测类别错误，预测框与 ground truth 的 IOU 大于 0.1。即预测的类别不正确，但预测框还勉强把目标给框住了。
*   **Background：**预测框与 ground truth 的 IOU 小于 0.1，即该预测框的位置为背景，没有目标。

Figure 4 显示了所有 20 个类中每种错误类型的平均细分情况![](https://img-blog.csdnimg.cn/966765b384bc45c59d88790f0d632836.png) 

**结论：**YOLO 定位错误率高于 Fast R-CNN；Fast R-CNN 背景预测错误率高于 YOLO 

### 4.3 Combining Fast R-CNN and YOLO—Fast R-CNN 与 YOLO 的结合

### 翻译

**YOLO 误认背景为目标的情况比 Fast R-CNN 少得多**。 通过使用 YOLO 消除 Fast R-CNN 的背景检测，我们获得了显著的性能提升。 对于 R-CNN 预测的每个边界框，我们检查 YOLO 是否预测了一个相似的框。 如果确实如此，那么我们会根据 YOLO 预测的概率和两个框之间的重叠情况提高预测值。

最好的 Fast R-CNN 模型在 VOC 2007 测试集中达到了 71.8％ 的 mAP。 当与 YOLO 合并时，其 mAP 增加了 3.2％ 至 75.0％。 我们还尝试将顶级 Fast R-CNN 模型与其他几个版本的 Fast R-CNN 结合起来。 这写的结合的平均增长率在 0.3％ 至 0.6％ 之间。

结合 YOLO 后获得的性能提高不仅仅是模型集成的副产品，因为结合不同版本的 Fast R-CNN 几乎没有什么益处。 相反，正是因为 YOLO 在测试时出现了各种各样的错误，所以它在提高 Fast R-CNN 的性能方面非常有效。

不幸的是，这种组合不会从 YOLO 的速度中受益，因为我们分别运行每个模型，然后合并结果。 但是，由于 YOLO 速度如此之快，与 Fast R-CNN 相比，它不会增加任何显著的计算时间。

### 精读

Table2 模型组合在 VOC 2007 上的实验结果对比![](https://img-blog.csdnimg.cn/4f796fba3a6f465e9cc3a62bcdbfc08c.png)

**结论：**因为 YOLO 在测试时犯了各种错误，所以它在提高快速 R-CNN 的性能方面非常有效。但是这种组合并不受益于 YOLO 的速度，由于 YOLO 很快，和 Fast R-CNN 相比，它不增加任何有意义的计算时间。

### 4.4 VOC 2012 Results—VOC 2012 结果 

### 翻译

在 VOC 2012 测试集中，YOLO 的 mAp 得分是 57.9％。这比现有最先进的技术水平低，更接近使用 VGG-16 的原始的 R-CNN，见表 3。与其最接近的竞争对手相比，我们的系统很难处理小物体上 (struggles with small objects)。在瓶子、羊、电视 / 监视器等类别上，YOLO 得分比 R-CNN 和 Feature Edit 低 8-10％。然而，在其他类别，如猫和火车 YOLO 取得了更好的表现。

我们的 Fast R-CNN + YOLO 模型组合是性能最高的检测方法之一。 Fast R-CNN 与 YOLO 的组合提高了 2.3％，在公共排行榜上提升了 5 个位置。

### 精读

Table 3 在 VOC2012 上 mAP 排序![](https://img-blog.csdnimg.cn/6729184943674516a7e1343d9ff411ff.png)

 **结论：**Fast R-CNN 从与 YOLO 的组合中得到 2.3% 的改进，在公共排行榜上提升了 5 个百分点。 

### 4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测 

### 翻译

用于目标检测的学术数据集的训练和测试数据是**服从同一分布**的。但在现实世界的应用中，很难预测所有可能的用例，他的测试数据可能与系统已经看到的不同 [3]。我们将 YOLO 与其他检测系统在毕加索(Picasso) 数据集 [12] 和人物艺术 (People-Art) 数据集 [3] 上进行了比较，这两个数据集用于测试艺术品上的人物检测。

作为参考 (for reference)，我们提供了 VOC 2007 的人形检测的 AP，其中所有模型仅在 VOC 2007 数据上训练。在 Picasso 数据集上测试的模型在是在 VOC 2012 上训练，而 People-Art 数据集上的模型则在 VOC 2010 上训练。

R-CNN 在 VOC 2007 上有很高的 AP 值。然而，当应用于艺术图像时，R-CNN 显着下降。R-CNN 使用选择性搜索来调整自然图像的候选边界框。R-CNN 在分类器阶段只能看到小区域，而且需要有很好的候选框。

DPM 在应用于艺术图像时可以很好地保持其 AP。之前的研究认为 DPM 表现良好，因为它具有强大的物体形状和布局空间模型。虽然 DPM 不会像 R-CNN 那样退化，但它的 AP 本来就很低。

YOLO 在 VOC 2007 上表现出色，其应用于艺术图像时其 AP 降低程度低于其他方法。与 DPM 一样，YOLO 模拟目标的大小和形状，以及目标之间的关系和目标通常出现的位置之间的关系。艺术图像和自然图像在像素级别上有很大不同，但它们在物体的大小和形状方面相似，因此 YOLO 仍然可以预测好的边界框和检测结果。

### 精读

Figure 5 通用性（Picasso 数据集和 People-Art 数据集）![](https://img-blog.csdnimg.cn/3b634920d5ba40b1b12577d2eb521dff.png) 

**结论：**YOLO 都具有很好的检测结果

**五、Real-Time Detection In The Wild—自然环境下的实时检测** 
-------------------------------------------------

###  翻译

YOLO 是一款快速，精确的物体检测器，非常适合计算机视觉应用。 我们将 YOLO 连接到网络摄像头，并验证它是否保持实时性能，包括从摄像头获取图像并显示检测结果的时间。

由此产生的系统是互动的和参与的。 虽然 YOLO 单独处理图像，但当连接到网络摄像头时，它的功能类似于跟踪系统，可在目标移动并在外观上发生变化时检测目标。 系统演示和源代码可在我们的项目网站上找到：[YOLO: Real-Time Object Detection](http://pjreddie.com/yolo/ "YOLO: Real-Time Object Detection")。

### 精读

![](https://img-blog.csdnimg.cn/047c59e479c8483fad0939c9d22b313a.png)

 **结论：**将 YOLO 连接到一个网络摄像头上，并验证它是否保持了实时性能，包括从摄像头中获取图像和显示检测结果的时间。结果证明效果很好，如上图所示，除了第二行第二个将人误判为飞机以外，别的没问题。

**六、Conclusion—结论**
-------------------

### **翻译**

我们介绍 YOLO——一种用于物体检测的统一模型。 我们的模型构造简单，可以直接在完整图像上训练。 与基于分类器的方法不同，YOLO 是通过与检测性能直接对应的损失函数进行训练的，并且整个模型是一起训练的。

快速 YOLO 是文献中最快的通用目标检测器，YOLO 推动实时对象检测的最新技术。 YOLO 还能很好地推广到新领域，使其成为快速，鲁棒性强的应用的理想选择。

### **精读**

####  到底什么是 YOLO？

*   YOLO 眼里目标检测是一个回归问题
*   一次性喂入图片，然后给出 bbox 和分类概率
*   简单来说，只看一次就知道图中物体的类别和位置

#### YOLO 过程总结：

**训练阶段:**

首先将一张图像分成 S × S 个 gird cell，然后将它一股脑送入 CNN，生成 S × S × (B × 5 + C）个结果，最后根据结果求 Loss 并反向传播梯度下降。

**预测、验证阶段：**

首先将一张图像分成 S × S 网格 (gird cell)，然后将它一股脑送入 CNN，生成 S × S × (B × 5 + C）个结果，最后用 NMS 选出合适的预选框。

本篇到这就结束了，我们 YOLOv2 见~